# Ava LLM Training Framework

> Production-ready framework for training state-of-the-art Mixture of Experts (MoE) language models with comprehensive stability enhancements, distributed training support, and advanced optimization techniques.

[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)]()
[![Version](https://img.shields.io/badge/Version-2.3.0-blue)]()
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)]()

---

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Training Pipeline Flowcharts](#training-pipeline-flowcharts)
- [Quick Start](#quick-start)
- [Model Sizes & Requirements](#model-sizes--requirements)
- [Key Features](#key-features)
- [Project Structure](#project-structure)
- [Configuration System](#configuration-system)
- [Training Stages](#training-stages)
- [Advanced Features](#advanced-features)
- [Troubleshooting](#troubleshooting)
- [Documentation](#documentation)

---

## Overview

Ava is a comprehensive LLM training framework implementing **8 phases of critical enhancements** for stable and efficient Mixture of Experts (MoE) model training. The framework contains ~32,772 lines of Python across 50+ active modules and is designed for both research and production use.

### Key Highlights

- **MoE Architecture**: Switch Transformer-based routing with 8 experts (configurable)
- **Production-Ready**: Battle-tested with comprehensive stability fixes
- **Distributed Training**: DeepSpeed integration with ZeRO optimization
- **RLHF Support**: Full Proximal Policy Optimization (PPO) pipeline
- **Advanced Optimization**: Adaptive learning rates, gradient surgery, quantization
- **Comprehensive Monitoring**: Real-time health dashboard and W&B integration

---

## Architecture

### High-Level System Architecture

```mermaid
graph TB
    subgraph "Data Pipeline"
        A[Raw Datasets] --> B[Data Download]
        B --> C[Data Processing]
        C --> D[Tokenization]
        D --> E[Training DataLoader]
    end

    subgraph "Model Architecture"
        F[Embedding Layer] --> G[Transformer Blocks]
        G --> H[MoE Router]
        H --> I1[Expert 1]
        H --> I2[Expert 2]
        H --> I3[Expert ...]
        H --> I8[Expert 8]
        I1 --> J[Output Layer]
        I2 --> J
        I3 --> J
        I8 --> J
    end

    subgraph "Training System"
        E --> K[Enhanced Trainer]
        K --> L[Gradient Health Monitor]
        K --> M[Adaptive LR Manager]
        K --> N[Memory Monitor]
        L --> O[DeepSpeed Engine]
        M --> O
        N --> O
    end

    subgraph "Optimization & Monitoring"
        O --> P[Loss Computation]
        P --> Q[MoE Balancing]
        Q --> R[Gradient Surgery]
        R --> S[Optimizer Step]
        S --> T[Checkpoint Manager]
        T --> U[W&B Logging]
    end

    subgraph "Output & Evaluation"
        U --> V[Model Checkpoints]
        V --> W[Generation Testing]
        V --> X[RLHF Fine-tuning]
        W --> Y[Evaluation Metrics]
        X --> Y
    end

    style A fill:#e1f5ff
    style F fill:#fff4e1
    style K fill:#ffe1f5
    style O fill:#e1ffe1
    style V fill:#f5e1ff
```

### Transformer Layer Architecture

```mermaid
graph LR
    A[Input Embeddings<br/>+ Position Embeddings] --> B[Layer Norm]
    B --> C[Multi-Head Attention<br/>8 heads, RoPE]
    C --> D[Residual Connection]
    D --> E[Layer Norm]
    E --> F[MoE Router]
    F --> G1[Expert 1<br/>FFN 512â†’2048â†’512]
    F --> G2[Expert 2<br/>FFN 512â†’2048â†’512]
    F --> G3[Expert ...<br/>...]
    F --> G8[Expert 8<br/>FFN 512â†’2048â†’512]
    G1 --> H[Load Balancing<br/>Sinkhorn-Knopp]
    G2 --> H
    G3 --> H
    G8 --> H
    H --> I[Residual Connection]
    I --> J[Output]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style F fill:#fce4ec
    style H fill:#f1f8e9
```

---

## Training Pipeline Flowcharts

### Complete Training Flow

```mermaid
flowchart TD
    Start([Start Training]) --> Init[Initialize Configuration]
    Init --> PreFlight{Pre-Flight Validation}

    PreFlight -->|Failed| ErrorReport[Generate Error Report]
    ErrorReport --> End([Exit])

    PreFlight -->|Passed| DataSetup[Setup Data Pipeline]
    DataSetup --> ModelInit[Initialize Model]
    ModelInit --> OptimizerSetup[Setup Optimizer & Scheduler]
    OptimizerSetup --> DeepSpeed[Initialize DeepSpeed Engine]

    DeepSpeed --> WarmupPhase[Warmup Phase<br/>3% of total steps]
    WarmupPhase --> TrainingLoop{Training Loop}

    TrainingLoop --> ForwardPass[Forward Pass]
    ForwardPass --> LossCalc[Loss Calculation<br/>+ MoE Balancing]
    LossCalc --> GradHealthCheck{Gradient Health OK?}

    GradHealthCheck -->|Unhealthy| ReduceLR[Reduce Learning Rate]
    ReduceLR --> BackwardPass
    GradHealthCheck -->|Healthy| BackwardPass[Backward Pass]

    BackwardPass --> GradSurgery{Gradient Surgery<br/>Enabled?}
    GradSurgery -->|Yes| ApplySurgery[Apply PCGrad/GradNorm]
    GradSurgery -->|No| GradClip
    ApplySurgery --> GradClip[Gradient Clipping]

    GradClip --> OptimizerStep[Optimizer Step]
    OptimizerStep --> MemCheck{Memory Check}

    MemCheck -->|>90% Usage| EmergencyClean[Emergency Cache Clear]
    MemCheck -->|OK| AdaptiveLR
    EmergencyClean --> AdaptiveLR{Adaptive LR<br/>Enabled?}

    AdaptiveLR -->|Yes + Plateau| BoostLR[Boost LR by 15%]
    AdaptiveLR -->|No Plateau| Logging
    BoostLR --> Logging[Log Metrics to W&B]

    Logging --> EvalCheck{Evaluation<br/>Step?}
    EvalCheck -->|Yes| Evaluate[Run Validation]
    EvalCheck -->|No| SaveCheck
    Evaluate --> SaveCheck{Save<br/>Checkpoint?}

    SaveCheck -->|Yes| SaveModel[Save Model + Metadata]
    SaveCheck -->|No| MoreSteps
    SaveModel --> MoreSteps{More Steps?}

    MoreSteps -->|Yes| TrainingLoop
    MoreSteps -->|No| FinalEval[Final Evaluation]

    FinalEval --> SaveFinal[Save Final Checkpoint]
    SaveFinal --> Complete([Training Complete])

    style Start fill:#4caf50,color:#fff
    style PreFlight fill:#ff9800,color:#fff
    style TrainingLoop fill:#2196f3,color:#fff
    style Complete fill:#4caf50,color:#fff
    style ErrorReport fill:#f44336,color:#fff
```

### Data Pipeline Flow

```mermaid
flowchart LR
    subgraph "Stage 1: Download"
        A[HuggingFace Datasets] --> B[unified_download.py]
        B --> C[Priority Tier 1<br/>10 datasets]
        B --> D[Priority Tier 2<br/>20 datasets]
        B --> E[Priority Tier 3<br/>20+ datasets]
    end

    subgraph "Stage 2: Processing"
        C --> F[Format Detection<br/>10-sample confidence]
        D --> F
        E --> F
        F --> G{Format Type?}
        G -->|JSONL| H[Parse JSONL]
        G -->|Arrow| I[Load Arrow]
        G -->|Parquet| J[Load Parquet]
        H --> K[Validation]
        I --> K
        J --> K
    end

    subgraph "Stage 3: Tokenization"
        K --> L[BPE Tokenizer<br/>65,536 vocab]
        L --> M[Multi-Column Support]
        M --> N[Quality Filters]
        N --> O[Min Length: 10 tokens]
        N --> P[Max Repetition: 60%]
        N --> Q[Max Consecutive: 10]
    end

    subgraph "Stage 4: Loading"
        O --> R[Training Split<br/>85%]
        P --> R
        Q --> R
        R --> S[Validation Split<br/>15%]
        R --> T[Streaming DataLoader]
        S --> T
        T --> U[Batch Formation<br/>Size: 12]
    end

    style A fill:#e1f5ff
    style F fill:#fff4e1
    style L fill:#ffe1f5
    style U fill:#e1ffe1
```

### RLHF Training Flow

```mermaid
flowchart TD
    Start([Start RLHF]) --> LoadPolicy[Load Policy Model<br/>from checkpoint]
    LoadPolicy --> LoadReward[Load/Initialize<br/>Reward Model]
    LoadReward --> LoadPrompts[Load Prompt Dataset]

    LoadPrompts --> EpochLoop{For Each Epoch}
    EpochLoop --> RolloutPhase[Rollout Phase]

    RolloutPhase --> GenResponses[Generate Responses<br/>for Batch of Prompts]
    GenResponses --> GetRewards[Compute Rewards<br/>Judge Model Scoring]
    GetRewards --> ComputeAdvantages[Compute Advantages<br/>GAE with Î»=0.95]

    ComputeAdvantages --> PPOLoop{PPO Update Loop<br/>4 epochs}

    PPOLoop --> ComputeRatio[Compute Probability Ratios<br/>Ï€_new/Ï€_old]
    ComputeRatio --> ClipRatio[Clip Ratios<br/>Îµ=0.2]
    ClipRatio --> PolicyLoss[Policy Loss<br/>PPO Objective]
    PolicyLoss --> ValueLoss[Value Loss<br/>MSE]
    ValueLoss --> EntropyBonus[Entropy Bonus<br/>coef=0.01]

    EntropyBonus --> TotalLoss[Total Loss]
    TotalLoss --> BackProp[Backward Pass<br/>Gradient Clipping: 0.5]
    BackProp --> OptimStep[Optimizer Step<br/>LR: 5e-7]

    OptimStep --> KLCheck{KL Divergence<br/>< 0.01?}
    KLCheck -->|No| AdjustKL[Adjust KL Penalty]
    KLCheck -->|Yes| NextMiniBatch
    AdjustKL --> NextMiniBatch{More Mini-Batches?}

    NextMiniBatch -->|Yes| PPOLoop
    NextMiniBatch -->|No| EvalCheck{Evaluation Step?}

    EvalCheck -->|Yes| EvalPrompts[Evaluate on<br/>50 Test Prompts]
    EvalCheck -->|No| SaveCheck
    EvalPrompts --> SaveCheck{Save Step?}

    SaveCheck -->|Yes| SaveCheckpoint[Save Policy + Value<br/>+ Metadata]
    SaveCheck -->|No| MoreRollouts
    SaveCheckpoint --> MoreRollouts{More Rollouts?}

    MoreRollouts -->|Yes| RolloutPhase
    MoreRollouts -->|No| MoreEpochs{More Epochs?}

    MoreEpochs -->|Yes| EpochLoop
    MoreEpochs -->|No| FinalSave[Save Final Model]
    FinalSave --> Complete([RLHF Complete])

    style Start fill:#4caf50,color:#fff
    style RolloutPhase fill:#2196f3,color:#fff
    style PPOLoop fill:#ff9800,color:#fff
    style Complete fill:#4caf50,color:#fff
```

### MoE Router Decision Flow

```mermaid
flowchart TD
    Input[Input Hidden States<br/>shape: batch Ã— seq Ã— hidden] --> RouterNet[Router Network<br/>Linear: hidden â†’ num_experts]
    RouterNet --> Logits[Router Logits<br/>shape: batch Ã— seq Ã— 8]

    Logits --> Jitter{Jitter Noise<br/>Enabled?}
    Jitter -->|Yes| AddNoise[Add Uniform Noise<br/>scale: 0.01]
    Jitter -->|No| Softmax
    AddNoise --> Softmax[Softmax Normalization]

    Softmax --> TopK[Select Top-2 Experts<br/>per token]
    TopK --> Scores[Expert Scores<br/>shape: batch Ã— seq Ã— 2]

    Scores --> Capacity{Check Capacity<br/>factor: 3.0}
    Capacity -->|Over| DropTokens[Drop Overflow Tokens]
    Capacity -->|Within| Route
    DropTokens --> Route[Route to Experts]

    Route --> Expert1[Expert 1<br/>FFN Processing]
    Route --> Expert2[Expert 2<br/>FFN Processing]

    Expert1 --> Combine[Weighted Combination<br/>output = Î£ score_i Ã— expert_i]
    Expert2 --> Combine

    Combine --> AuxLoss[Compute Auxiliary Loss<br/>Load Balancing]
    AuxLoss --> Balance{Load Balanced?}

    Balance -->|No| Penalty[Add Balancing Penalty<br/>coef: 0.12]
    Balance -->|Yes| Output
    Penalty --> Output[Output Hidden States]

    style Input fill:#e3f2fd
    style TopK fill:#fff3e0
    style Route fill:#fce4ec
    style AuxLoss fill:#f1f8e9
    style Output fill:#e8f5e9
```

### Adaptive Learning Rate Flow

```mermaid
flowchart TD
    Start[Training Step] --> CheckInterval{LR Check<br/>Interval?<br/>every 100 steps}
    CheckInterval -->|No| Continue[Continue Training]
    CheckInterval -->|Yes| ComputeWindow[Compute Loss Window<br/>last 100 batches]

    ComputeWindow --> ComputeImprovement[Calculate Improvement<br/>vs best loss]
    ComputeImprovement --> ImprovementCheck{Improvement ><br/>0.005?}

    ImprovementCheck -->|Yes| UpdateBest[Update Best Loss]
    ImprovementCheck -->|No| IncrementPlateau[Increment Plateau Counter]

    UpdateBest --> ResetPlateau[Reset Plateau Counter]
    ResetPlateau --> Continue

    IncrementPlateau --> PlateauCheck{Plateau Counter ><br/>1500 steps?}
    PlateauCheck -->|No| Continue
    PlateauCheck -->|Yes| StabilityCheck{Check Stability<br/>last 5 gradients}

    StabilityCheck -->|Unstable| ReduceLR[Reduce LR<br/>factor: 0.7]
    StabilityCheck -->|Stable| CurrentLR{Current LR<br/>< max_lr?}

    ReduceLR --> ApplyLR[Apply New LR]
    CurrentLR -->|No| MaxReached[At Max LR]
    CurrentLR -->|Yes| BoostLR[Boost LR<br/>factor: 1.15]

    MaxReached --> Continue
    BoostLR --> CheckBounds{Within Bounds?<br/>min: 0.00003<br/>max: 0.0002}

    CheckBounds -->|No| ClampLR[Clamp to Bounds]
    CheckBounds -->|Yes| ApplyLR
    ClampLR --> ApplyLR

    ApplyLR --> LogChange[Log LR Change<br/>to W&B]
    LogChange --> ResetPlateau2[Reset Plateau Counter]
    ResetPlateau2 --> Continue

    style Start fill:#e3f2fd
    style PlateauCheck fill:#fff3e0
    style BoostLR fill:#c8e6c9
    style ReduceLR fill:#ffccbc
    style Continue fill:#f5f5f5
```

### Memory Management Flow

```mermaid
flowchart TD
    TrainStep[Training Step] --> MemCheck[Check GPU Memory]
    MemCheck --> Usage{Memory Usage}

    Usage -->|< 75%| Normal[Normal Operation]
    Usage -->|75-80%| Warning[âš ï¸ Warning Level]
    Usage -->|80-85%| Critical[ğŸ”¶ Critical Level]
    Usage -->|85-90%| Emergency[ğŸ”´ Emergency Level]
    Usage -->|> 90%| Crisis[ğŸ’€ Crisis Level]

    Normal --> Continue[Continue Training]

    Warning --> Log1[Log Warning]
    Log1 --> Continue

    Critical --> Log2[Log Critical Alert]
    Log2 --> Reduce1{Can Reduce<br/>Batch Size?}
    Reduce1 -->|Yes| ReduceBatch1[Reduce Batch by 25%]
    Reduce1 -->|No| Continue
    ReduceBatch1 --> Continue

    Emergency --> ClearCache1[torch.cuda.empty_cache]
    ClearCache1 --> GC1[gc.collect]
    GC1 --> Reduce2{Can Reduce<br/>Batch Size?}
    Reduce2 -->|Yes| ReduceBatch2[Reduce Batch by 50%]
    Reduce2 -->|No| Continue
    ReduceBatch2 --> Continue

    Crisis --> ClearCache2[torch.cuda.empty_cache]
    ClearCache2 --> GC2[gc.collect]
    GC2 --> SyncCUDA[torch.cuda.synchronize]
    SyncCUDA --> Reduce3{Can Reduce<br/>Batch Size?}
    Reduce3 -->|Yes| MinBatch[Set to Minimum Batch: 4]
    Reduce3 -->|No| Checkpoint[Save Emergency Checkpoint]
    MinBatch --> Checkpoint
    Checkpoint --> RaiseOOM[Raise OOM Error]
    RaiseOOM --> GracefulShutdown[Graceful Shutdown]

    style Normal fill:#c8e6c9
    style Warning fill:#fff9c4
    style Critical fill:#ffcc80
    style Emergency fill:#ef9a9a
    style Crisis fill:#e57373
    style GracefulShutdown fill:#424242,color:#fff
```

---

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/ava-llm.git
cd ava-llm

# Install dependencies
pip install -r requirements.txt

# Install PyTorch with CUDA support (adjust for your CUDA version)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Option 1: Train from Scratch (Recommended)

```bash
# Download and prepare data
python code/scripts/1_data_download/unified_download.py
python code/scripts/2_data_prep/process_all_data.py

# Train tokenizer (optional - pre-trained available)
python code/scripts/3_tokenizer/train_tokenizer.py

# Start training
python code/scripts/5_training/train.py --config code/configs/gpu/small.yaml
```

### Option 2: Resume from Checkpoint

```bash
python code/scripts/5_training/train.py \
  --config code/configs/gpu/small.yaml \
  --resume code/outputs/runs/latest/checkpoints/best_model.pt
```

### Option 3: RLHF Fine-tuning

```bash
python code/scripts/6_rhlf_Finetuning/train_rlhf.py \
  --config code/configs/gpu/small.yaml
```

### Test Generation

```bash
python code/scripts/7_generation/generate.py \
  --model code/outputs/runs/latest/checkpoints/best_model.pt \
  --prompt "Once upon a time"
```

---

## Model Sizes & Requirements

| Size | Parameters | Hidden Size | Layers | Heads | Experts | Memory (Training) | Memory (Inference) | Speed | Use Case |
|------|-----------|-------------|--------|-------|---------|-------------------|-------------------|-------|----------|
| **tiny** | 100M | 384 | 10 | 6 | 4 | 4-8 GB | 2-4 GB | âš¡âš¡âš¡ | Development, Testing |
| **small** | 233M | 512 | 14 | 8 | 8 | 8-12 GB | 4-6 GB | âš¡âš¡ | **Production Default** |
| **base** | 500M | 768 | 18 | 12 | 8 | 16-24 GB | 8-12 GB | âš¡ | Research, High Quality |
| **large** | 1.3B | 1024 | 24 | 16 | 16 | 24-40 GB | 12-20 GB | ğŸŒ | Maximum Quality |

### Hardware Recommendations

**Minimum Requirements:**
- GPU: NVIDIA RTX 3060 (12GB VRAM) for `tiny`
- RAM: 16GB
- Storage: 50GB free space
- CUDA: 11.8+

**Recommended Setup:**
- GPU: NVIDIA RTX 3090 Ti (24GB VRAM) for `small`
- RAM: 32GB
- Storage: 100GB SSD
- CUDA: 12.0+

**Production Setup:**
- GPU: NVIDIA A100 (40GB/80GB) for `base`/`large`
- RAM: 64GB+
- Storage: 500GB NVMe SSD
- Multi-GPU: DeepSpeed with ZeRO-2/3

---

## Key Features

### Model Architecture

- **MoE Design**: Switch Transformer with configurable experts (4-16)
- **Attention**: Multi-head attention with Rotary Position Embeddings (RoPE)
- **Expert Routing**: DeepSeek router with top-K selection
- **Load Balancing**: Sinkhorn-Knopp algorithm with auxiliary loss
- **Activation**: GELU with optional Mixture of Activations (MoA)

### Training Enhancements (8 Phases)

1. **Stability Fixes**
   - Gradient health monitoring with explosion detection
   - Adaptive gradient clipping (warmup from 2.0 to final value)
   - Loss health tracking (NaN/Inf detection and recovery)
   - Emergency memory management

2. **Data Pipeline Fixes**
   - Multi-format support (.jsonl, .arrow, .parquet)
   - 10-sample confidence-based format detection
   - Corruption handling with graceful fallbacks
   - Minimum samples validation

3. **Training Loop Fixes**
   - Percentage-based LR warmup (default 3% of steps)
   - Adaptive LR with plateau detection
   - Automatic LR boost on plateaus (15% increase)
   - Cosine schedule with restarts

4. **Distributed Fixes**
   - Collective OOM detection across ranks
   - Synchronized checkpointing with barriers
   - Rank-aware error handling
   - Graceful distributed cleanup

5. **Progressive Training**
   - Sequence length scaling (128 â†’ 2048)
   - Dynamic batch sizing with GPU monitoring
   - Curriculum learning with difficulty scoring
   - Binary search OOM recovery

6. **Feature Compatibility**
   - Compatibility validation matrix
   - Feature conflict detection (critical/error/warning)
   - Dependency checking
   - Pre-flight validation reports

7. **Observability**
   - Hierarchical logging (training/evaluation/errors/debug)
   - Real-time health dashboard
   - Comprehensive metrics (gradient norms, expert utilization)
   - W&B integration with automatic logging

8. **Testing & Validation**
   - Pre-flight checks (GPU, data, config)
   - Continuous monitoring during training
   - Checkpoint resume smoke tests
   - Integration test framework

### Optimization Features

- **Mixed Precision**: FP16/BF16/FP8 support
- **Quantization**: INT4/INT8 weight quantization with NVFP4
- **Gradient Surgery**: PCGrad and GradNorm for multi-task learning
- **Flash Attention**: Memory-efficient attention (optional)
- **Torch Compile**: JIT compilation with max-autotune
- **DeepSpeed**: ZeRO stages 1/2/3 with CPU/NVMe offloading

### Advanced Features

- **Adaptive Multi-Token Prediction (MTP)**: Predict multiple future tokens
- **Retrieval-Augmented Generation (RAG)**: Knowledge base integration
- **Mixture of Heads (MoH)**: Dynamic attention head selection
- **Mixture of Activations (MoA)**: Per-layer activation function selection
- **Episodic Memory**: Experience replay for continual learning
- **Custom Tokenizers**: BPE tokenizer with 65,536 vocabulary

---

## Project Structure

```
/project/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”‚   â”œâ”€â”€ gpu/                   # GPU-specific configs
â”‚   â”‚   â”‚   â”œâ”€â”€ tiny.yaml         # 100M model
â”‚   â”‚   â”‚   â”œâ”€â”€ small.yaml        # 233M model (default)
â”‚   â”‚   â”‚   â”œâ”€â”€ base.yaml         # 500M model
â”‚   â”‚   â”‚   â””â”€â”€ large.yaml        # 1.3B model
â”‚   â”‚   â”œâ”€â”€ distributed/           # DeepSpeed configs
â”‚   â”‚   â”œâ”€â”€ hardware/              # Hardware-specific (A100, H100)
â”‚   â”‚   â””â”€â”€ research/              # Experimental features
â”‚   â”‚
â”‚   â”œâ”€â”€ src/Ava/                   # Core framework (32K+ LOC)
â”‚   â”‚   â”œâ”€â”€ models/               # Model architectures
â”‚   â”‚   â”‚   â”œâ”€â”€ adaptive_mtp_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ confidence_gate.py
â”‚   â”‚   â”‚   â””â”€â”€ prediction_heads.py
â”‚   â”‚   â”œâ”€â”€ training/             # Training utilities (21 modules)
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_trainer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ adaptive_lr.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gradient_health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_monitor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ progressive_training.py
â”‚   â”‚   â”‚   â”œâ”€â”€ run_manager.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ losses/               # Loss functions (6 types)
â”‚   â”‚   â”‚   â”œâ”€â”€ unified_loss.py
â”‚   â”‚   â”‚   â”œâ”€â”€ deepseek_loss.py
â”‚   â”‚   â”‚   â”œâ”€â”€ anti_repetition_loss.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ data/                 # Data loading
â”‚   â”‚   â”‚   â””â”€â”€ dataloader.py
â”‚   â”‚   â”œâ”€â”€ generation/           # Text generation
â”‚   â”‚   â”‚   â””â”€â”€ generator.py
â”‚   â”‚   â””â”€â”€ optimization/         # Optimizers & quantization
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                  # Training & evaluation scripts
â”‚   â”‚   â”œâ”€â”€ 1_data_download/
â”‚   â”‚   â”‚   â””â”€â”€ unified_download.py
â”‚   â”‚   â”œâ”€â”€ 2_data_prep/
â”‚   â”‚   â”‚   â””â”€â”€ process_all_data.py
â”‚   â”‚   â”œâ”€â”€ 3_tokenizer/
â”‚   â”‚   â”‚   â””â”€â”€ train_tokenizer.py
â”‚   â”‚   â”œâ”€â”€ 4_lr_finding/
â”‚   â”‚   â”‚   â””â”€â”€ run_lr_finder_enhanced.py
â”‚   â”‚   â”œâ”€â”€ 5_training/
â”‚   â”‚   â”‚   â””â”€â”€ train.py          # Main training script
â”‚   â”‚   â”œâ”€â”€ 6_rhlf_Finetuning/
â”‚   â”‚   â”‚   â””â”€â”€ train_rlhf.py
â”‚   â”‚   â”œâ”€â”€ 7_generation/
â”‚   â”‚   â”‚   â””â”€â”€ generate.py
â”‚   â”‚   â””â”€â”€ evaluation/
â”‚   â”‚       â”œâ”€â”€ measure_coherence.py
â”‚   â”‚       â””â”€â”€ test_generation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                     # Training data
â”‚   â”‚   â”œâ”€â”€ processed/            # Tokenized datasets
â”‚   â”‚   â”œâ”€â”€ combined/             # Combined datasets
â”‚   â”‚   â””â”€â”€ rlhf/                 # RLHF prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # Model artifacts
â”‚   â”‚   â””â”€â”€ tokenizer/
â”‚   â”‚       â””â”€â”€ enhanced-65536/   # Pre-trained tokenizer
â”‚   â”‚
â”‚   â””â”€â”€ outputs/                  # Training outputs
â”‚       â””â”€â”€ runs/
â”‚           â””â”€â”€ run_YYYYMMDD_HHMMSS_<id>/
â”‚               â”œâ”€â”€ checkpoints/
â”‚               â”‚   â”œâ”€â”€ best_model.pt
â”‚               â”‚   â”œâ”€â”€ latest_model.pt
â”‚               â”‚   â””â”€â”€ step_N/
â”‚               â”œâ”€â”€ logs/
â”‚               â”‚   â”œâ”€â”€ training.log
â”‚               â”‚   â”œâ”€â”€ evaluation.log
â”‚               â”‚   â”œâ”€â”€ errors.log
â”‚               â”‚   â””â”€â”€ debug.log
â”‚               â”œâ”€â”€ configs/
â”‚               â”‚   â””â”€â”€ config.yaml
â”‚               â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ claude_docs/                  # Comprehensive documentation
â”‚   â”œâ”€â”€ CODEBASE_OVERVIEW.md
â”‚   â”œâ”€â”€ LOGGING_FIX_SUMMARY.md
â”‚   â”œâ”€â”€ OOM_FIX_GUIDE.md
â”‚   â”œâ”€â”€ SPIKE_ANALYSIS.md
â”‚   â””â”€â”€ TECHNICAL_ANALYSIS.md
â”‚
â””â”€â”€ README.md                     # This file
```

---

## Configuration System

### Configuration Hierarchy

```mermaid
graph TD
    A[Base Config<br/>small.yaml] --> B[Model Config]
    A --> C[Training Config]
    A --> D[Data Config]
    A --> E[Optimization Config]

    B --> B1[Architecture<br/>hidden_size, layers, experts]
    B --> B2[Attention<br/>heads, dropout, RoPE]

    C --> C1[Hyperparameters<br/>LR, batch size, epochs]
    C --> C2[Adaptive LR<br/>plateau detection, boost]
    C --> C3[DeepSpeed<br/>ZeRO stage, offloading]

    D --> D1[Data Sources<br/>paths, formats]
    D --> D2[Preprocessing<br/>tokenization, filtering]

    E --> E1[Quantization<br/>NVFP4, INT8]
    E --> E2[Performance<br/>torch compile, flash attn]

    style A fill:#4caf50,color:#fff
    style B fill:#2196f3,color:#fff
    style C fill:#ff9800,color:#fff
    style D fill:#9c27b0,color:#fff
    style E fill:#f44336,color:#fff
```

### Key Configuration Parameters

**Model Architecture:**
```yaml
model:
  hidden_size: 512              # Embedding dimension
  num_layers: 14                # Transformer layers
  num_attention_heads: 8        # Attention heads
  intermediate_size: 2048       # FFN hidden size
  num_experts: 8                # MoE experts
  num_experts_per_token: 2      # Top-K routing
  expert_capacity_factor: 3.0   # Load balancing
  router_aux_loss_coef: 0.12    # Balancing loss weight
```

**Training:**
```yaml
training:
  batch_size: 12                    # Per-device batch size
  gradient_accumulation_steps: 4    # Effective batch: 48
  learning_rate: 0.00015            # Base learning rate
  lr_scheduler_type: cosine_with_restarts
  num_cycles: 2                     # LR restart cycles
  warmup_steps: 1000                # Warmup duration
  max_gradient_norm: 2.0            # Gradient clipping

  adaptive_lr:
    enabled: true                   # Enable adaptive LR
    plateau_patience: 1500          # Steps before LR boost
    plateau_factor: 1.15            # LR boost multiplier
    min_lr: 0.00003                 # Minimum LR
    max_lr: 0.0002                  # Maximum LR
```

**Data:**
```yaml
data:
  data_dir: /project/code/data/processed
  max_length: 256                   # Sequence length
  tokenizer_name: /project/code/models/tokenizer/enhanced-65536
  val_split_ratio: 0.15             # Validation split
  min_sequence_length: 10           # Quality filter
  max_sequence_repetition_rate: 0.6
```

### Configuration Examples

**High-Quality Training (Slow but Better):**
```yaml
training:
  batch_size: 8
  learning_rate: 0.0001
  mixed_precision: bf16

model:
  num_experts: 16
  expert_capacity_factor: 4.0
  attention_dropout: 0.05
```

**Fast Training (Development):**
```yaml
training:
  batch_size: 16
  mixed_precision: fp16

  performance:
    ultra_fast_mode: true
    torch_compile:
      enabled: true
      mode: reduce-overhead
```

**Memory-Constrained (8GB GPU):**
```yaml
training:
  batch_size: 4
  gradient_accumulation_steps: 12

  deepspeed:
    zero_stage: 2
    cpu_offload: true

  memory:
    enable_cpu_offload: true
    clear_cache_frequency: 10
```

---

## Training Stages

### Stage 1: Data Download

Downloads 50+ datasets from HuggingFace in priority tiers:

```bash
python code/scripts/1_data_download/unified_download.py
```

**Priority Tiers:**
- **Tier 1** (10 datasets): High-quality conversational data
- **Tier 2** (20 datasets): General instruction-following
- **Tier 3** (20+ datasets): Domain-specific and specialized

### Stage 2: Data Preparation

Processes and validates all downloaded data:

```bash
python code/scripts/2_data_prep/process_all_data.py
```

**Features:**
- Auto-format detection (JSONL, Arrow, Parquet)
- Quality filtering (length, repetition)
- Multi-column dataset support
- Corruption handling

### Stage 3: Tokenizer Training

Trains a custom BPE tokenizer with 65,536 vocabulary:

```bash
python code/scripts/3_tokenizer/train_tokenizer.py
```

**Pre-trained tokenizer available:** Skip this step if using default.

### Stage 4: LR Finding

Automatically finds optimal learning rate:

```bash
python code/scripts/4_lr_finding/run_lr_finder_enhanced.py \
  --config code/configs/gpu/small.yaml
```

**Features:**
- Range: 1e-8 to 0.01
- FastAI suggestion method
- Savitzky-Golay filtering
- Steepest descent detection

### Stage 5: Training

Main training with all 8 phases of enhancements:

```bash
python code/scripts/5_training/train.py \
  --config code/configs/gpu/small.yaml
```

**Monitoring:** View real-time metrics at https://wandb.ai

### Stage 6: RLHF Fine-tuning

Proximal Policy Optimization (PPO) fine-tuning:

```bash
python code/scripts/6_rhlf_Finetuning/train_rlhf.py \
  --config code/configs/gpu/small.yaml
```

**Features:**
- Judge model for reward scoring
- Advantage estimation (GAE)
- KL divergence penalty
- Adaptive KL coefficient

### Stage 7: Generation & Evaluation

Test model generation quality:

```bash
python code/scripts/7_generation/generate.py \
  --model code/outputs/runs/latest/checkpoints/best_model.pt \
  --prompt "Once upon a time" \
  --temperature 0.8 \
  --top_p 0.9 \
  --max_length 200
```

**Evaluation metrics:**
- Perplexity
- Distinct-2 (diversity)
- Coherence score
- Repetition rate
- Entropy

---

## Advanced Features

### Adaptive Multi-Token Prediction (MTP)

Predicts multiple future tokens simultaneously:

```yaml
training:
  enhanced_features:
    losses:
      use_multi_token_prediction: true
      num_future_tokens: 3
      mtp_weight: 0.1

    adaptive_mtp:
      use_adaptive_mtp: true
      num_prediction_heads: 3
      confidence_threshold_train: 0.6
```

**Benefits:**
- Faster inference (multiple tokens per forward pass)
- Better long-range dependencies
- Improved generation quality

### Retrieval-Augmented Generation (RAG)

Integrate external knowledge base:

```yaml
training:
  enhanced_features:
    rag:
      enabled: true
      knowledge_base_path: /path/to/vector_db
      max_retrieved_docs: 5
      rag_fusion_type: concatenate
```

### Mixture of Heads (MoH)

Dynamic attention head selection:

```yaml
training:
  enhanced_features:
    architecture:
      use_moh: true
```

### Quantization

Reduce model size with minimal quality loss:

```yaml
training:
  enhanced_features:
    quantization:
      quantization_aware: true
      use_nvfp4: true              # NVIDIA FP4 format
      nvfp4_mode: weight_only       # or "weight_activation"
      bit_width: 4
```

**Results:**
- 4x smaller model size
- 2-3x faster inference
- <1% perplexity degradation

### Gradient Surgery

Optimize multi-task learning:

```yaml
training:
  enhanced_features:
    gradient:
      gradient_surgery: true
      gradient_surgery_method: pcgrad  # or "gradnorm"
```

---

## Troubleshooting

### Common Issues & Solutions

#### Issue: Out of Memory (OOM)

**Symptoms:**
```
RuntimeError: CUDA out of memory
```

**Solutions:**
1. Reduce batch size:
   ```yaml
   training:
     batch_size: 8  # or lower
   ```

2. Enable gradient checkpointing:
   ```yaml
   output:
     gradient_checkpointing: true
   ```

3. Use DeepSpeed ZeRO-2 or ZeRO-3:
   ```yaml
   training:
     deepspeed:
       zero_stage: 2
       cpu_offload: true
   ```

4. Enable CPU offloading:
   ```yaml
   training:
     memory:
       enable_cpu_offload: true
   ```

#### Issue: Loss Spikes

**Symptoms:**
- Sudden loss increases during training
- Training divergence

**Solutions:**
1. Enable gradient health monitoring (default):
   ```yaml
   training:
     gradient_health:
       enabled: true
       explosion_threshold: 3.0
       auto_reduce_lr: true
   ```

2. Reduce learning rate:
   ```yaml
   training:
     learning_rate: 0.0001  # lower
   ```

3. Increase warmup steps:
   ```yaml
   training:
     warmup_steps: 2000  # longer warmup
   ```

#### Issue: Poor Generation Quality

**Symptoms:**
- Repetitive text
- Incoherent output
- Mode collapse

**Solutions:**
1. Adjust generation parameters:
   ```yaml
   generation:
     temperature: 0.9        # increase randomness
     top_p: 0.92             # nucleus sampling
     repetition_penalty: 1.5  # penalize repetition
     no_repeat_ngram_size: 3
   ```

2. Enable anti-repetition loss:
   ```yaml
   training:
     repetition_penalty_weight: 0.5
     immediate_repetition_weight: 1.0
   ```

3. Increase entropy regularization:
   ```yaml
   training:
     entropy_regularization: 0.2
     output_diversity_weight: 1.0
   ```

#### Issue: Slow Training

**Symptoms:**
- Low GPU utilization
- Slow steps per second

**Solutions:**
1. Enable ultra-fast mode:
   ```yaml
   training:
     performance:
       ultra_fast_mode: true
       torch_compile:
         enabled: true
         mode: max-autotune
   ```

2. Increase batch size:
   ```yaml
   training:
     batch_size: 16
   ```

3. Reduce dataloader workers if CPU-bound:
   ```yaml
   training:
     dataloader_num_workers: 4
   ```

#### Issue: Training Plateau

**Symptoms:**
- Loss stops decreasing
- Validation metrics stagnant

**Solutions:**
1. Enable adaptive LR (default):
   ```yaml
   training:
     adaptive_lr:
       enabled: true
       plateau_patience: 1500
       plateau_factor: 1.15
   ```

2. Use cosine schedule with restarts:
   ```yaml
   training:
     lr_scheduler_type: cosine_with_restarts
     num_cycles: 2
   ```

3. Reduce batch size for stronger gradient signal:
   ```yaml
   training:
     batch_size: 8
   ```

### Debug Checklist

- [ ] **GPU Available:** `nvidia-smi` shows GPU
- [ ] **CUDA Version:** PyTorch CUDA version matches system CUDA
- [ ] **Data Path:** Data directory exists and contains .jsonl/.arrow files
- [ ] **Tokenizer:** Tokenizer path is valid
- [ ] **Disk Space:** Sufficient space for checkpoints (20GB+)
- [ ] **Memory:** Batch size fits in GPU memory
- [ ] **Config Valid:** YAML syntax is correct
- [ ] **Dependencies:** All packages installed (`pip list`)

### Emergency Recovery

**Save checkpoint before crash:**
```python
# Training automatically saves emergency checkpoint on OOM
# Resume with:
python code/scripts/5_training/train.py \
  --config code/configs/gpu/small.yaml \
  --resume code/outputs/runs/latest/checkpoints/emergency.pt
```

**Reset optimizer state:**
```bash
# If optimizer state is corrupted, reset it:
python code/scripts/5_training/train.py \
  --config code/configs/gpu/small.yaml \
  --resume code/outputs/runs/latest/checkpoints/best_model.pt \
  --reset-optimizer
```

---

## Documentation

### ğŸ“š Complete Documentation Hub

**[â†’ Full Documentation Index](code/docs/README.md)** - All documentation organized by topic

### ğŸš€ Quick Start Documentation

1. **[QUICK_REFERENCE.md](code/docs/QUICK_REFERENCE.md)** â­ **START HERE!**
   - Fast decision trees for all common tasks
   - Command cheat sheets
   - Optimizer/loss/config selection guides
   - Troubleshooting quick fixes

2. **[01_ARCHITECTURE.md](code/docs/01_ARCHITECTURE.md)**
   - Complete system architecture with diagrams
   - MoE model structure and components
   - Layer-by-layer breakdown

3. **[02_TRAINING_GUIDE.md](code/docs/02_TRAINING_GUIDE.md)**
   - End-to-end training walkthrough
   - Configuration and setup
   - Monitoring and debugging

### ğŸ“– Comprehensive Guides with Flowcharts

4. **[04_LOSS_FUNCTIONS.md](code/docs/04_LOSS_FUNCTIONS.md)**
   - UnifiedLoss system architecture
   - Loss composition flowcharts
   - Configuration examples for all scenarios

5. **[05_OPTIMIZATION_GUIDE.md](code/docs/05_OPTIMIZATION_GUIDE.md)**
   - Optimizer comparison (Lion, Sophia, AdaFactor)
   - Learning rate management pipelines
   - Gradient operations and monitoring

6. **[06_EVALUATION_GENERATION.md](code/docs/06_EVALUATION_GENERATION.md)**
   - Evaluation metrics and workflows
   - Text generation strategies
   - RLHF training pipeline

7. **[07_CONFIGURATION_SYSTEM.md](code/docs/07_CONFIGURATION_SYSTEM.md)**
   - Configuration hierarchy and override system
   - Hardware-specific configs (A100, H100)
   - Distributed training setup (DeepSpeed ZeRO)

### ğŸ”§ Specialized Guides

8. **[03_MEMORY_OPTIMIZATION.md](code/docs/03_MEMORY_OPTIMIZATION.md)**
   - Out-of-memory solutions
   - Memory optimization strategies
   - Emergency recovery procedures

9. **[FLOWCHARTS_VISUAL.md](code/docs/FLOWCHARTS_VISUAL.md)**
   - Visual flowcharts with emoji annotations
   - Training pipeline visualization
   - Component interaction diagrams

10. **[LOSSES_README.md](code/docs/LOSSES_README.md)** & **[LOSSES_USAGE_GUIDE.md](code/docs/LOSSES_USAGE_GUIDE.md)**
    - Detailed loss function reference
    - Usage examples and patterns

### ğŸ“‚ Legacy Documentation

Located in [`claude_docs/`](claude_docs/):

- **[CODEBASE_OVERVIEW.md](claude_docs/CODEBASE_OVERVIEW.md)** - System architecture overview
- **[LOGGING_FIX_SUMMARY.md](claude_docs/LOGGING_FIX_SUMMARY.md)** - Logging system details
- **[OOM_FIX_GUIDE.md](claude_docs/OOM_FIX_GUIDE.md)** - Memory troubleshooting (see new guide above)
- **[SPIKE_ANALYSIS.md](claude_docs/SPIKE_ANALYSIS.md)** - Loss spike investigation
- **[TECHNICAL_ANALYSIS.md](claude_docs/TECHNICAL_ANALYSIS.md)** - Technical analysis

### API Documentation

**Key Classes:**

```python
from Ava.training import EnhancedTrainer
from Ava.models import EnhancedMoEModel
from Ava.data import MultiColumnDataLoader
from Ava.losses import UnifiedLoss
from Ava.training import AdaptiveLRManager

# Initialize trainer
trainer = EnhancedTrainer(
    model=model,
    config=config,
    train_dataloader=train_dl,
    val_dataloader=val_dl
)

# Train with all enhancements
trainer.train()
```

### Configuration Reference

See [code/configs/gpu/small.yaml](code/configs/gpu/small.yaml) for complete parameter documentation with inline comments.

---

## Performance Benchmarks

### Training Speed

| Model | GPU | Batch Size | Tokens/sec | Steps/sec | Memory |
|-------|-----|------------|------------|-----------|--------|
| tiny | RTX 3090 Ti | 16 | 24,576 | 6.0 | 8 GB |
| small | RTX 3090 Ti | 12 | 18,432 | 4.0 | 12 GB |
| base | A100 40GB | 16 | 49,152 | 4.0 | 32 GB |
| large | A100 80GB | 8 | 32,768 | 2.0 | 64 GB |

### Generation Quality

**Metrics after 30K steps (small model):**
- **Perplexity**: 8.2
- **Distinct-2**: 0.73 (high diversity)
- **Coherence**: 68/100 (moderate)
- **Repetition Rate**: 0.18 (low)

### Memory Efficiency

**DeepSpeed ZeRO-2 vs Standard:**
- **Model Size**: 233M parameters
- **Standard Training**: 18 GB VRAM
- **ZeRO-2**: 12 GB VRAM (-33%)
- **ZeRO-3 + CPU Offload**: 8 GB VRAM (-56%)

---

## Contributing

We welcome contributions! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** changes (`git commit -m 'Add amazing feature'`)
4. **Push** to branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black code/src/Ava/
isort code/src/Ava/

# Type checking
mypy code/src/Ava/
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Citation

If you use this framework in your research, please cite:

```bibtex
@software{ava_llm_2025,
  title={Ava: Production-Ready MoE Language Model Training Framework},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/ava-llm}
}
```

---

## Acknowledgments

- **DeepSpeed** - Distributed training optimization
- **HuggingFace** - Transformers library and datasets
- **Weights & Biases** - Experiment tracking
- **PyTorch** - Deep learning framework

---

## Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/ava-llm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/ava-llm/discussions)
- **Email**: support@example.com

---

## Roadmap

### Version 2.4.0 (Q2 2025)

- [ ] Multi-modal support (vision + language)
- [ ] Improved RLHF with DPO (Direct Preference Optimization)
- [ ] Automatic hyperparameter tuning
- [ ] Model compression (pruning + distillation)
- [ ] Extended context length (4K â†’ 32K tokens)

### Version 3.0.0 (Q4 2025)

- [ ] Full distributed training across clusters
- [ ] Cloud integration (AWS, GCP, Azure)
- [ ] Web UI for training monitoring
- [ ] Pre-trained checkpoints release
- [ ] Production inference server

---

<div align="center">

**Built with â¤ï¸ for the AI research community**

[â­ Star on GitHub](https://github.com/yourusername/ava-llm) | [ğŸ“– Documentation](claude_docs/) | [ğŸ› Report Bug](https://github.com/yourusername/ava-llm/issues)

</div>
