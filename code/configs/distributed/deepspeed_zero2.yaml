# DeepSpeed ZeRO-2 Configuration
# Optimizer states + gradients sharding across GPUs
# Target: 4-16 GPUs, large models
# Memory savings: ~8x for optimizer + gradients

model:
  # Large Architecture for ZeRO-2
  hidden_size: 1024
  num_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  vocab_size: 50257
  max_position_embeddings: 2048

  # MoE Configuration
  num_experts: 32
  num_experts_per_token: 4
  expert_capacity_factor: 1.5
  router_type: "switch"

  # Enhanced Features (more enabled)
  use_moh: true
  use_moa: true
  use_rag: true
  use_cross_attention: true
  num_cross_attention_layers: 4
  use_episodic_memory: true
  memory_size: 2000

  # DeepSpeed compatibility
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: true
  deepspeed_moe_param_groups: true

training:
  batch_size: 2
  gradient_accumulation_steps: 16
  max_gradient_norm: 1.0
  num_epochs: 5
  warmup_steps: 2000
  learning_rate: 8e-5
  weight_decay: 0.01
  mixed_precision: "bf16"
  gradient_checkpointing: true

# DeepSpeed Configuration (ZeRO-2 Focus)
deepspeed:
  enabled: true
  zero_stage: 2              # Optimizer + gradient sharding
  cpu_offload: false
  nvme_offload: false
  precision: "bf16"
  gradient_accumulation_steps: 16
  train_batch_size: 32       # 2 * 16
  micro_batch_size: 2
  activation_checkpointing: true
  partition_activations: true
  gradient_clipping: 1.0

  # ZeRO-2 specific settings
  zero_allow_untested_optimizer: true
  zero_reduce_scatter: true
  zero_overlap_comm: true
  zero_contiguous_gradients: true
  zero_reduce_bucket_size: 500000000    # 500MB
  zero_allgather_bucket_size: 500000000

# Enhanced Features (Full Suite)
enhanced_features:
  architecture:
    use_moh: true
    use_moa: true
    use_cross_attention: true
    expert_routing_type: "switch"

  rag:
    enabled: true
    knowledge_base_path: "data/knowledge_base"
    max_retrieved_docs: 5

  losses:
    focal_loss: true
    contrastive_loss: true
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true

  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: true
    gradient_surgery_method: "cagrad"

  memory:
    use_episodic_memory: true
    memory_capacity: 2000
    memory_selection_strategy: "importance"
    memory_replay_ratio: 0.2

  quantization:
    quantization_aware: true
    use_nvfp4: true
    bit_width: 4

# Data Loading
data:
  data_dir: "/project/code/data/processed"
  max_length: 2048
  tokenizer_name: "gpt2"
  train_batch_size: 2
  eval_batch_size: 4

  # Data loading configuration
  streaming: true
  buffer_size: 10000
  distributed: true
  multi_column: true

# Performance & Memory
performance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: false

memory:
  enable_memory_pool: true
  pool_size_gb: 20.0
  gradient_checkpointing: true
  memory_threshold_gb: 20.0
  clear_cache_frequency: 50

# Output & Evaluation
output:
  output_dir: "outputs/deepspeed_zero2"
  save_total_limit: 2

evaluation:
  eval_during_training: true
  eval_metrics: "perplexity,bleu,rouge"
  comprehensive_eval: true

# Monitoring
wandb:
  enabled: true
  project: "ava-moe-deepspeed-zero2"
  tags: ["deepspeed", "zero2", "distributed", "large-model", "rag"]
  notes: "DeepSpeed ZeRO-2 with optimizer+gradient sharding, RAG enabled"
  group: "deepspeed-zero2"

# Hardware Requirements
# GPUs: 4-16 GPUs with 20GB+ VRAM each
# Total VRAM: 80-320GB
# Network: High-bandwidth interconnect required (NVLink/InfiniBand)
# Use case: Large models (1B+ parameters) with gradient memory concerns