# DeepSpeed ZeRO-1 Configuration
# Optimizer state sharding across GPUs
# Target: 2-8 GPUs, medium-large models
# Memory savings: ~4x for optimizer states

model:
  # Medium-Large Architecture for ZeRO-1
  hidden_size: 768
  num_layers: 20
  num_attention_heads: 12
  intermediate_size: 3072
  vocab_size: 50257
  max_position_embeddings: 2048

  # MoE Configuration
  num_experts: 16
  num_experts_per_token: 2
  expert_capacity_factor: 1.25
  router_type: "switch"

  # Enhanced Features (moderate)
  use_moh: true
  use_moa: false
  use_rag: false
  use_cross_attention: true
  num_cross_attention_layers: 2
  use_episodic_memory: true
  memory_size: 1000

  # DeepSpeed compatibility
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: false
  deepspeed_moe_param_groups: true

training:
  batch_size: 4
  gradient_accumulation_steps: 8
  max_gradient_norm: 1.0
  num_epochs: 10
  warmup_steps: 1000
  learning_rate: 1e-4
  weight_decay: 0.01
  mixed_precision: "bf16"
  gradient_checkpointing: true

# DeepSpeed Configuration (ZeRO-1 Focus)
deepspeed:
  enabled: true
  zero_stage: 1              # Optimizer state sharding only
  cpu_offload: false
  nvme_offload: false
  precision: "bf16"
  gradient_accumulation_steps: 8
  train_batch_size: 32       # 4 * 8
  micro_batch_size: 4
  activation_checkpointing: true
  partition_activations: false
  gradient_clipping: 1.0

  # ZeRO-1 specific settings
  zero_allow_untested_optimizer: true
  zero_force_ds_cpu_optimizer: false
  zero_reduce_scatter: true
  zero_overlap_comm: true
  zero_contiguous_gradients: true

# Enhanced Features
enhanced_features:
  architecture:
    use_moh: true
    use_moa: false
    use_cross_attention: true
    expert_routing_type: "switch"

  losses:
    focal_loss: true
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true

  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: false
    gradient_surgery_method: "pcgrad"

  memory:
    use_episodic_memory: true
    memory_capacity: 1000
    memory_selection_strategy: "importance"

  quantization:
    quantization_aware: true
    use_nvfp4: false
    bit_width: 8

# Data Loading
data:
  data_dir: "/project/code/data/processed"
  max_length: 2048
  tokenizer_name: "gpt2"
  train_batch_size: 4
  eval_batch_size: 8

  # Data loading configuration
  streaming: true
  buffer_size: 5000
  distributed: true

# Performance & Memory
performance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: true

memory:
  enable_memory_pool: true
  pool_size_gb: 16.0
  gradient_checkpointing: true
  memory_threshold_gb: 16.0
  clear_cache_frequency: 100

# Monitoring
wandb:
  enabled: true
  project: "ava-moe-deepspeed-zero1"
  tags: ["deepspeed", "zero1", "distributed", "multi-gpu"]
  notes: "DeepSpeed ZeRO-1 configuration with optimizer state sharding"
  group: "deepspeed-zero1"

# Hardware Requirements
# GPUs: 2-8 GPUs with 16GB+ VRAM each
# Total VRAM: 32-128GB
# Network: High-bandwidth interconnect (NVLink, InfiniBand)
# Use case: Medium-large models with optimizer memory concerns