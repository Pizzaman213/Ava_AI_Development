# DeepSpeed ZeRO-3 Configuration
# Full parameter + optimizer + gradient sharding across GPUs
# Target: 8+ GPUs, ultra-large models
# Memory savings: 64x+ for model parameters

model:
  # Ultra-Large Architecture for ZeRO-3
  hidden_size: 1536
  num_layers: 32
  num_attention_heads: 24
  intermediate_size: 6144
  vocab_size: 50257
  max_position_embeddings: 4096

  # MoE Configuration
  num_experts: 64
  num_experts_per_token: 6
  expert_capacity_factor: 2.0
  router_type: "gshard"

  # Enhanced Features (all enabled)
  use_moh: true
  use_moa: true
  use_rag: true
  use_cross_attention: true
  num_cross_attention_layers: 8
  use_episodic_memory: true
  memory_size: 5000

  # DeepSpeed compatibility
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: true
  deepspeed_moe_param_groups: true

training:
  batch_size: 1
  gradient_accumulation_steps: 32
  max_gradient_norm: 1.0
  num_epochs: 3
  warmup_steps: 5000
  learning_rate: 5e-5
  weight_decay: 0.01
  mixed_precision: "bf16"
  gradient_checkpointing: true

# DeepSpeed Configuration (ZeRO-3 Focus)
deepspeed:
  enabled: true
  zero_stage: 3              # Full parameter sharding
  cpu_offload: true          # Enable CPU offloading for ultra-large models
  nvme_offload: false        # Can enable for even larger models
  precision: "bf16"
  gradient_accumulation_steps: 32
  train_batch_size: 32       # 1 * 32
  micro_batch_size: 1
  activation_checkpointing: true
  partition_activations: true
  gradient_clipping: 1.0

  # ZeRO-3 specific settings
  zero_allow_untested_optimizer: true
  zero_reduce_scatter: true
  zero_overlap_comm: true
  zero_contiguous_gradients: true
  zero_stage3_prefetch_bucket_size: 500000000      # 500MB
  zero_stage3_param_persistence_threshold: 1000000 # 1M params
  zero_stage3_max_live_parameters: 1000000000      # 1B params
  zero_stage3_max_reuse_distance: 1000000000       # 1B params
  zero_stage3_gather_16bit_weights_on_model_save: true

  # CPU offloading configuration
  offload_optimizer:
    device: "cpu"
    pin_memory: true
  offload_param:
    device: "cpu"
    pin_memory: true

# Enhanced Features (Everything Enabled)
enhanced_features:
  architecture:
    use_moh: true
    use_moa: true
    use_cross_attention: true
    use_alibi: true
    expert_routing_type: "gshard"

  rag:
    enabled: true
    knowledge_base_path: "data/knowledge_base"
    max_retrieved_docs: 10
    rag_fusion_type: "attention"

  losses:
    focal_loss: true
    contrastive_loss: true
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true

  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: true
    gradient_surgery_method: "cagrad"

  memory:
    use_episodic_memory: true
    memory_capacity: 5000
    memory_selection_strategy: "importance"
    memory_replay_ratio: 0.3
    memory_replay_strategy: "importance"

  quantization:
    quantization_aware: true
    use_nvfp4: true
    bit_width: 4
    stochastic_rounding: true
    use_hadamard_transform: true

# Data Loading (Advanced)
data:
  data_dir: "/project/code/data/processed"
  max_length: 4096
  tokenizer_name: "gpt2"
  train_batch_size: 1
  eval_batch_size: 2

  # Data loading configuration
  streaming: true
  buffer_size: 20000
  distributed: true
  multi_column: true

# Performance & Memory (Aggressive)
performance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: false
  no_sync: false

memory:
  enable_memory_pool: true
  pool_size_gb: 30.0
  gradient_checkpointing: true
  memory_threshold_gb: 30.0
  enable_cpu_offload: true
  clear_cache_frequency: 25

# Output & Evaluation
output:
  output_dir: "outputs/deepspeed_zero3"
  save_total_limit: 1
  save_on_each_node: false

evaluation:
  eval_during_training: true
  eval_metrics: "perplexity,bleu,rouge,bertscore"
  comprehensive_eval: true

# Run Management
run_management:
  experiment_name: "ava-ultra-large-zero3"
  resume_from_checkpoint: null

# Monitoring
wandb:
  enabled: true
  project: "ava-moe-deepspeed-zero3"
  tags: ["deepspeed", "zero3", "ultra-large", "cpu-offload", "all-features"]
  notes: "DeepSpeed ZeRO-3 with full parameter sharding, CPU offloading, all features"
  group: "deepspeed-zero3"
  job_type: "train"

# Hardware Requirements
# GPUs: 8+ GPUs with 16GB+ VRAM each (can work with less due to CPU offloading)
# CPU RAM: 256GB+ recommended for parameter offloading
# Total VRAM: 128GB+ (much less needed due to parameter sharding)
# Network: Very high-bandwidth interconnect essential (NVLink, InfiniBand)
# Storage: Fast SSD for CPU offloading, NVMe for parameter offloading
# Use case: Ultra-large models (10B+ parameters) that don't fit in GPU memory