model:
  hidden_size: 1024
  num_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  vocab_size: 65536  # Enhanced custom tokenizer vocab size
  max_position_embeddings: 2048
  num_experts: 32
  num_experts_per_token: 4
  expert_capacity_factor: 1.5
  router_type: switch
  router_aux_loss_coef: 0.01
  router_jitter_noise: 0.0
  use_cache: true
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  layer_norm_eps: 1e-5
  initializer_range: 0.02
  hidden_act: gelu
  use_moh: true
  use_moa: true
  use_rag: true
  use_cross_attention: true
  num_cross_attention_layers: 4
  use_episodic_memory: true
  memory_size: 2000
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: true
  deepspeed_moe_param_groups: true
  tie_word_embeddings: false
training:
  batch_size: 2
  gradient_accumulation_steps: 16
  max_gradient_norm: 5.0
  num_epochs: 5
  max_steps: 50000  # CRITICAL FIX: Limit to prevent over-training and LR collapse (~15-20 epochs)
  warmup_steps: 2000
  warmup_schedule: cosine
  learning_rate: 0.00042
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  lr_scheduler_type: cosine
  lr_end: 1.0e-04  # CRITICAL FIX: Add minimum LR to prevent collapse
  # Repetition penalty weights (Fix #2 - addresses " time time time" issue)
  repetition_penalty_weight: 0.5      # N-gram repetition penalty weight
  immediate_repetition_weight: 1.0    # Token-to-token repetition weight
  min_sequence_length: 20             # Minimum tokens before allowing EOS
  eos_penalty_weight: 5.0             # Penalty for early EOS tokens
  mixed_precision: fp16
  dataloader_num_workers: 8
  gradient_checkpointing: true
  eval_strategy: steps
  eval_steps: 200
  save_strategy: steps
  save_steps: 500
  logging_steps: 25
  early_stopping_patience: 5
  early_stopping_threshold: 0.005
  use_ema: true
  ema_decay: 0.999
  label_smoothing: 0.1
  gradient_health:
    enabled: true
    initial_clip_value: 2.0
    final_clip_value: 0.5
    warmup_steps: 2000  # Matches training.warmup_steps for consistency
    explosion_threshold: 5.0
    explosion_window: 5
    auto_reduce_lr: true
    lr_reduction_factor: 0.5
data:
  data_dir: /project/code/data/processed
  max_length: 2048
  max_train_examples: -1
  max_eval_examples: 2000
  tokenizer_name: /project/code/models/tokenizer/enhanced-65k  # Custom enhanced tokenizer
  padding_side: right
  truncation: true
  train_batch_size: 2
  eval_batch_size: 4
  dataloader_drop_last: true
  dataloader_pin_memory: true

  # Data loading configuration
  streaming: true
  buffer_size: 10000
  max_samples: -1
  multi_column: true
  use_multi_column: false
  distributed: true
  use_weighted_mixing: true
  mixing_temperature: 1.0
  shuffle_seed: 42
  reshuffle_each_epoch: true
  drop_last_batch: true
  prefetch_factor: 2
  persistent_workers: true
device: auto
fp16: true
bf16: false
gradient_checkpointing: true
use_cpu_offload: false
pin_memory: true
wandb:
  enabled: true
  project: ava-moe-gpu-large-enhanced
  tags:
  - gpu
  - large
  - 1.5B
  - production
  - enhanced
  - deepspeed
  - rag
  - nvfp4
  - custom-tokenizer
  - enhanced-65k
  - 65k-vocab
  - weighted-mixing
  - pre-tokenized
  entity: null
  name: null
  notes: 'Enhanced with: Custom 65k tokenizer (math/code optimized), weighted data mixing
    (DoReMi), pre-tokenized data'
  group: large-models
  job_type: train
  offline: false
  cache_size: 2000
  cache_flush_interval: 25
compilation:
  enabled: false
  backend: inductor
quantization:
  enabled: false
  bits: 8
deepspeed:
  use_deepspeed: true
  zero_stage: 2
  cpu_offload: false
  nvme_offload: false
  precision_type: bf16
  train_batch_size: 32
  micro_batch_size: 2
  gradient_accumulation_steps: 16
  activation_checkpointing: true
enhanced_features:
  architecture:
    use_moh: true
    use_moa: true
    use_cross_attention: true
    use_alibi: false
    expert_routing_type: switch
  rag:
    enabled: true
    knowledge_base_path: data/knowledge_base
    max_retrieved_docs: 5
    rag_fusion_type: concatenate
  losses:
    focal_loss: true
    contrastive_loss: true
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true
  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: true
    gradient_surgery_method: pcgrad
    gradient_norm_tracking: true
    gradient_anomaly_detection: true
  memory:
    use_episodic_memory: true
    memory_capacity: 2000
    memory_selection_strategy: importance
    memory_importance_threshold: 0.6
    memory_adaptation_rate: 0.01
    memory_performance_window: 200
    memory_replay_ratio: 0.2
    memory_replay_strategy: importance
  quantization:
    quantization_aware: true
    use_nvfp4: true
    bit_width: 4
    nvfp4_block_size: 64
    stochastic_rounding: true
    use_hadamard_transform: trueperformance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: true
  no_sync: false
memory:
  enable_memory_pool: true
  pool_size_gb: 20.0
  gradient_checkpointing: true
  memory_threshold_gb: 20.0
  enable_cpu_offload: false
  clear_cache_frequency: 50
evaluation:
  eval_during_training: true
  eval_metrics: perplexity,bleu,rouge
  comprehensive_eval: true
output:
  output_dir: outputs/large
  save_total_limit: 3
  save_on_each_node: false
run_management:
  experiment_name: ava-large-enhanced
  run_id: null
  resume_from_checkpoint: null
distributed:
  enabled: true
  backend: nccl
stability:
  loss_smoothing: true
  loss_smoothing_alpha: 0.9
  lr_warmup_ratio: 0.1
  lr_decay_ratio: 0.1
  sync_gradients_each_step: false
  use_kahan_summation: true
  use_stable_embedding: true
  stop_on_nan: true
  max_grad_norm_before_stop: 100.0
  max_loss_before_stop: 20.0
generation:
  repetition_penalty: 1.2
  eos_penalty: 0.5
  min_length: 20
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  use_ngram_blocking: false
  ngram_size: 3
  no_repeat_ngram_size: 0
  do_sample: true
  num_beams: 1
lr_finder:
  enabled: false
  start_lr: 1.0e-08
  end_lr: 0.01
  num_iterations: 500
  suggestion_method: fastai
  use_suggested_lr: false
  use_savgol_filter: true
  savgol_window: 31
  beta: 0.9
  momentum_cycling: false
  track_validation: false
  num_runs: 1
  gradient_accumulation_steps: 16


# RLHF Fine-tuning Configuration
rlhf:
  # Model paths
  policy_model_path: /project/code/outputs/runs/latest/model  # Path to pretrained model to finetune
  judge_model_path: /project/code/outputs/runs/latest/model  # Path to judge model (can be same as policy)
  reward_model_path: null  # Optional: path to pre-trained reward model

  # Reward model settings
  use_model_to_model_reward: true  # Use model-to-model rating (judge evaluates policy outputs)
  freeze_reward_model: true  # Freeze reward model during training

  # Dataset paths
  prompt_dataset_path: /project/code/data/rlhf/prompts.json  # Path to prompts dataset
  eval_prompt_dataset_path: /project/code/data/rlhf/eval_prompts.json  # Optional: evaluation prompts

  # Training settings
  num_epochs: 2  # Fewer epochs for large model
  rollout_batch_size: 8  # Smaller batch for large model
  num_rollouts_per_epoch: 1000  # Number of rollouts per epoch
  max_prompt_length: 2048

  # Output settings
  save_dir: outputs/rlhf/large
  log_dir: logs/rlhf/large
  save_every: 250  # Save more frequently for large model
  eval_every: 125  # Evaluate more frequently
  num_eval_prompts: 50  # Number of prompts to use for evaluation

  # Device
  device: cuda  # cuda or cpu

  # Weights & Biases logging
  use_wandb: true
  wandb_project: ava-rlhf
  wandb_name: rlhf-large-v1

  # PPO configuration
  ppo:
    # Learning rate and optimization
    learning_rate: 3.0e-7  # Very low LR for large model RLHF
    batch_size: 8  # PPO batch size
    mini_batch_size: 2  # Mini-batch size for PPO updates
    gradient_accumulation_steps: 8  # More gradient accumulation for large model
    max_grad_norm: 0.3  # Very conservative gradient clipping

    # PPO hyperparameters
    ppo_epochs: 4  # Number of PPO epochs per batch
    clip_range: 0.2  # PPO clipping parameter
    clip_range_value: 0.2  # Value function clipping
    value_loss_coef: 0.5  # Weight for value loss
    entropy_coef: 0.01  # Entropy bonus coefficient
    gamma: 1.0  # Discount factor (1.0 for language modeling)
    lambda: 0.95  # GAE lambda

    # KL divergence penalty
    kl_penalty: kl  # 'kl', 'abs', or 'mse'
    target_kl: 0.005  # Stricter KL for large model
    init_kl_coef: 0.3  # Higher initial KL coefficient
    adaptive_kl: true  # Adaptively adjust KL coefficient

    # Generation settings
    max_gen_length: 256  # Maximum generation length
    temperature: 1.0  # Sampling temperature
    top_k: 50  # Top-k sampling
    top_p: 0.95  # Nucleus sampling

    # Training schedule
    num_train_epochs: 1
    max_steps: null  # Optional: maximum number of steps
    warmup_steps: 50  # Fewer warmup steps for large model
    logging_steps: 10
    save_steps: 250
    eval_steps: 125

    # Reward processing
    use_score_scaling: true  # Scale rewards
    use_score_norm: true  # Normalize rewards
    whiten_rewards: true  # Whiten advantages
    clip_reward: 10.0  # Clip rewards to [-clip_reward, clip_reward]

    # Memory optimization
    gradient_checkpointing: true  # Essential for large model
    mixed_precision: bf16  # 'no', 'fp16', or 'bf16'
