model:
  hidden_size: 768
  num_layers: 16
  num_attention_heads: 12
  intermediate_size: 3072
  vocab_size: 65536  # Enhanced custom tokenizer vocab size
  max_position_embeddings: 2048
  num_experts: 16
  num_experts_per_token: 2
  expert_capacity_factor: 1.25
  router_type: switch
  router_aux_loss_coef: 0.01
  router_jitter_noise: 0.0
  use_cache: true
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  layer_norm_eps: 1e-5
  initializer_range: 0.02
  hidden_act: gelu
  use_moh: false
  use_moa: false
  use_rag: false
  use_cross_attention: false
  use_episodic_memory: false
  tie_word_embeddings: false
training:
  batch_size: 8
  gradient_accumulation_steps: 4
  max_gradient_norm: 0.5
  num_epochs: 10
  max_steps: 40000  # CRITICAL FIX: Limit to prevent over-training and LR collapse (~12-15 epochs)
  warmup_steps: 2000
  warmup_schedule: cosine
  learning_rate: 0.00042
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  lr_scheduler_type: cosine
  lr_end: 1.0e-04  # CRITICAL FIX: Add minimum LR to prevent collapse
  # Repetition penalty weights (Fix #2 - addresses " time time time" issue)
  repetition_penalty_weight: 0.5      # N-gram repetition penalty weight
  immediate_repetition_weight: 1.0    # Token-to-token repetition weight
  min_sequence_length: 20             # Minimum tokens before allowing EOS
  eos_penalty_weight: 5.0             # Penalty for early EOS tokens
  mixed_precision: fp16
  dataloader_num_workers: 8
  gradient_checkpointing: true
  eval_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: 1000
  logging_steps: 50
  early_stopping_patience: 3
  early_stopping_threshold: 0.01
  gradient_health:
    enabled: true
    initial_clip_value: 2.0
    final_clip_value: 0.5
    warmup_steps: 2000  # Matches training.warmup_steps for consistency
    explosion_threshold: 5.0
    explosion_window: 5
    auto_reduce_lr: true
    lr_reduction_factor: 0.5
data:
  data_dir: /project/code/data/processed
  max_length: 1024
  max_train_examples: -1
  max_eval_examples: 1000
  tokenizer_name: /project/code/models/tokenizer/enhanced-65k  # Custom enhanced tokenizer
  padding_side: right
  truncation: true
  train_batch_size: 8
  eval_batch_size: 16
  dataloader_drop_last: true
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2
  dataloader_persistent_workers: true

  # Data loading configuration
  use_weighted_mixing: true
  mixing_temperature: 1.0
  streaming: true
  buffer_size: 15000
  max_samples: -1
  shuffle_seed: 42
  reshuffle_each_epoch: true
  drop_last_batch: true
  prefetch_factor: 2
  persistent_workers: true
  multi_column: false
  use_multi_column: false
  distributed: false
  num_workers: 8
  enable_bucketing: false
  bucket_boundaries:
  - 256
  - 512
  - 1024
  max_bucket_size: 32
device: auto
fp16: true
bf16: false
wandb:
  enabled: true
  project: ava-moe-gpu-base
  tags:
  - gpu
  - base
  - 500M
  - standard
  - custom-tokenizer
  - enhanced-65k
  - 65k-vocab
  - weighted-mixing
  - pre-tokenized
  notes: 'Enhanced with: Custom 65k tokenizer (math/code optimized), weighted data mixing
    (DoReMi), pre-tokenized data'
stability:
  loss_smoothing: true
  loss_smoothing_alpha: 0.9
  lr_warmup_ratio: 0.1
  lr_decay_ratio: 0.1
  sync_gradients_each_step: false
  use_kahan_summation: true
  use_stable_embedding: true
  stop_on_nan: true
  max_grad_norm_before_stop: 100.0
  max_loss_before_stop: 20.0performance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: false
  no_sync: false
  torch_compile:
    enabled: false
    mode: default
    fullgraph: false
    dynamic: false
    backend: inductor
memory:
  enable_memory_pool: false
  pool_size_gb: 16.0
  gradient_checkpointing: true
  memory_threshold_gb: 16.0
  enable_cpu_offload: false
  clear_cache_frequency: 1000
  silent_mode: true
  target_utilization: 0.90
  warning_threshold: 0.92
  critical_threshold: 0.95
  emergency_threshold: 0.98
evaluation:
  eval_during_training: true
  eval_metrics: perplexity
  comprehensive_eval: false
  moe_metrics:
    track_expert_utilization: false
    track_routing_entropy: false
    track_load_balance: false
    track_expert_specialization: false
    log_frequency: 50000
output:
  output_dir: outputs/base
  save_total_limit: 3
  save_on_each_node: false
run_management:
  experiment_name: ava-base-enhanced
  run_id: null
  resume_from_checkpoint: null
lr_finder:
  enabled: false
  start_lr: 1.0e-08
  end_lr: 0.01
  num_iterations: 500
  suggestion_method: fastai
  use_suggested_lr: false
  use_savgol_filter: true
  savgol_window: 31
  beta: 0.9
  momentum_cycling: false
  track_validation: false
  num_runs: 1
  gradient_accumulation_steps: 4
generation:
  repetition_penalty: 1.2
  eos_penalty: 0.5
  min_length: 20
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  use_ngram_blocking: false
  ngram_size: 3
  no_repeat_ngram_size: 0
  do_sample: true
  num_beams: 1
enhanced_features:
  architecture:
    use_moh: false
    use_moa: false
    use_cross_attention: false
    use_alibi: false
    expert_routing_type: switch
  rag:
    enabled: false
    knowledge_base_path: null
    max_retrieved_docs: 5
    rag_fusion_type: concatenate
  losses:
    use_multi_token_prediction: false
    num_future_tokens: 1
    mtp_weight: 0.01
    initial_temperature: 1.0
    adaptive_temperature: false
    label_smoothing: 0.0
    use_moe_balancing: true
    gradient_balance_weight: 0.01
    focal_loss: false
    contrastive_loss: false
    diversity_loss: false
    auxiliary_loss: true
    adaptive_loss_scaling: false
    contrastive_loss_weight: 0.0
    diversity_loss_weight: 0.0
  gradient:
    gradient_surgery: false
    adaptive_gradient_surgery: false
    gradient_surgery_method: pcgrad
    gradient_norm_tracking: false
    gradient_anomaly_detection: false
  memory:
    use_episodic_memory: false
    memory_capacity: 500
    memory_selection_strategy: importance
    memory_importance_threshold: 0.7
    memory_adaptation_rate: 0.01
    silent_mode: true
  quantization:
    quantization_aware: false
    use_nvfp4: false
    bit_width: 8
    nvfp4_block_size: 64
    stochastic_rounding: false
deepspeed:
  use_deepspeed: false
  zero_stage: 0
  cpu_offload: false
  nvme_offload: false
  precision_type: fp16
  train_batch_size: 32
  micro_batch_size: 8
  gradient_accumulation_steps: 4
  activation_checkpointing: true


# RLHF Fine-tuning Configuration
rlhf:
  # Model paths
  policy_model_path: /project/code/outputs/runs/latest/model  # Path to pretrained model to finetune
  judge_model_path: /project/code/outputs/runs/latest/model  # Path to judge model (can be same as policy)
  reward_model_path: null  # Optional: path to pre-trained reward model

  # Reward model settings
  use_model_to_model_reward: true  # Use model-to-model rating (judge evaluates policy outputs)
  freeze_reward_model: true  # Freeze reward model during training

  # Dataset paths
  prompt_dataset_path: /project/code/data/rlhf/prompts.json  # Path to prompts dataset
  eval_prompt_dataset_path: /project/code/data/rlhf/eval_prompts.json  # Optional: evaluation prompts

  # Training settings
  num_epochs: 3
  rollout_batch_size: 16  # Number of prompts to process at once
  num_rollouts_per_epoch: 800  # Number of rollouts per epoch
  max_prompt_length: 1024

  # Output settings
  save_dir: outputs/rlhf/base
  log_dir: logs/rlhf/base
  save_every: 500  # Save checkpoint every N steps
  eval_every: 250  # Evaluate every N steps
  num_eval_prompts: 50  # Number of prompts to use for evaluation

  # Device
  device: cuda  # cuda or cpu

  # Weights & Biases logging
  use_wandb: true
  wandb_project: ava-rlhf
  wandb_name: rlhf-base-v1

  # PPO configuration
  ppo:
    # Learning rate and optimization
    learning_rate: 8.0e-7  # Low LR for RLHF fine-tuning on base model
    batch_size: 16  # PPO batch size
    mini_batch_size: 4  # Mini-batch size for PPO updates
    gradient_accumulation_steps: 4  # Gradient accumulation
    max_grad_norm: 0.5  # Lower gradient clipping for stability

    # PPO hyperparameters
    ppo_epochs: 4  # Number of PPO epochs per batch
    clip_range: 0.2  # PPO clipping parameter
    clip_range_value: 0.2  # Value function clipping
    value_loss_coef: 0.5  # Weight for value loss
    entropy_coef: 0.01  # Entropy bonus coefficient
    gamma: 1.0  # Discount factor (1.0 for language modeling)
    lambda: 0.95  # GAE lambda

    # KL divergence penalty
    kl_penalty: kl  # 'kl', 'abs', or 'mse'
    target_kl: 0.01  # Target KL divergence
    init_kl_coef: 0.2  # Initial KL coefficient
    adaptive_kl: true  # Adaptively adjust KL coefficient

    # Generation settings
    max_gen_length: 256  # Maximum generation length
    temperature: 1.0  # Sampling temperature
    top_k: 50  # Top-k sampling
    top_p: 0.95  # Nucleus sampling

    # Training schedule
    num_train_epochs: 1
    max_steps: null  # Optional: maximum number of steps
    warmup_steps: 100  # Warmup steps
    logging_steps: 10
    save_steps: 500
    eval_steps: 250

    # Reward processing
    use_score_scaling: true  # Scale rewards
    use_score_norm: true  # Normalize rewards
    whiten_rewards: true  # Whiten advantages
    clip_reward: 10.0  # Clip rewards to [-clip_reward, clip_reward]

    # Memory optimization
    gradient_checkpointing: true
    mixed_precision: fp16  # 'no', 'fp16', or 'bf16'
