model:
  hidden_size: 512
  num_layers: 14
  num_attention_heads: 8
  intermediate_size: 2048
  vocab_size: 65536
  max_position_embeddings: 128
  num_experts: 8
  num_experts_per_token: 2
  expert_capacity_factor: 3.0
  router_type: deepseek
  router_aux_loss_coef: 0.12
  router_jitter_noise: 0.01
  expert_dropout: 0.0
  use_cache: true
  rope_theta: 10000
  attention_dropout: 0.12
  hidden_dropout: 0.12
  use_flash_attention: true
  layer_norm_eps: 0.00001
  initializer_range: 0.01
  hidden_act: gelu
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: false
  deepspeed_moe_param_groups: true
  tie_word_embeddings: false
  dropout: 0.12

training:
  batch_size: 64  # REDUCED: Smaller batches = stronger gradient signal for plateau breaking
  gradient_accumulation_steps: 2  # Effective batch = 48 (reduced from 64)
  max_gradient_norm: 2.0
  num_epochs: 1
  max_steps: null
  warmup_steps: 1000  # Extended warmup for stability
  warmup_schedule: linear
  learning_rate: 0.00008  # DECREASED: Lower LR for fine-tuning after plateau
  lr_scheduler_type: cosine_with_restarts  # NEW: Restarts help escape plateaus
  num_cycles: 2  # NEW: Two cosine cycles for learning rate restarts
  lr_end: 0.00001  # DECREASED: Lower end LR for stable convergence
  weight_decay: 0.08
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  repetition_penalty_weight: 0
  immediate_repetition_weight: 0
  min_sequence_length: 1
  eos_penalty_weight: 2
  eos_logit_bias: -1.5
  entropy_regularization: 0.1
  output_diversity_weight: 0.5
  mixed_precision: fp16
  dataloader_num_workers: 6  # OPTIMIZED: Reduced for better CPU cache locality
  eval_strategy: steps
  eval_steps: 500  # REDUCED: More frequent evaluation to catch improvements
  eval_steps_type: training_steps
  save_strategy: steps
  save_steps: 1000
  logging_steps: 10
  early_stopping_patience: 5  # INCREASED: Give more time before stopping
  early_stopping_threshold: 0.005  # REDUCED: More sensitive to improvements
  adam_betas:
    - 0.9
    - 0.95
  no_decay_patterns:
    - bias
    - LayerNorm.weight
    - layernorm.weight
    - ln_f.weight
    - ln_
    - norm.weight
  
  adaptive_lr:
    enabled: true  # ENABLED: Dynamically adjust LR on plateaus
    min_lr: 0.00001  # Don't go below this (adjusted for lower base LR)
    max_lr: 0.00012  # Can boost up to this (adjusted for lower base LR)
    min_improvement: 0.005  # Require 0.005 improvement
    plateau_patience: 1500  # Wait 1500 steps before adjusting
    plateau_factor: 1.15  # Boost LR by 15% on plateau
    lr_check_interval: 100
    batch_loss_window: 100
    stability_threshold: 5
    increase_factor: 1.15  # Increase by 15% when stuck
    divergence_threshold: 5
    emergency_factor: 0.7
    warmup_percentage: 0.05  # 5% warmup for adaptive changes
  
  progressive:
    enable_progressive_training: false
    enable_sequence_scaling: false
    initial_seq_length: 128
    final_seq_length: 128
    length_schedule: exponential
    length_growth_epochs: 1000
    enable_curriculum: false
    curriculum_metric: loss
    enable_dynamic_batch: false
  
  dynamic_batching:
    enabled: false
    min_batch_size: 8
    max_batch_size: 8
    target_memory_utilization: 0.85
    adjustment_frequency: 100
    adjustment_factor: 1
    warmup_steps: 500
    smooth_transitions: false
  
  gradient_health:
    enabled: true
    initial_clip_value: 2.0
    final_clip_value: 2.0
    warmup_steps: 1000
    explosion_threshold: 3.0
    explosion_window: 5
    auto_reduce_lr: true
    lr_reduction_factor: 0.5
  
  enhanced_features:
    architecture:
      use_moh: false
      use_moa: false
      use_cross_attention: true
      use_alibi: false
      expert_routing_type: deepseek
    
    rag:
      enabled: false
      knowledge_base_path: null
      max_retrieved_docs: 5
      rag_fusion_type: concatenate
    
    losses:
      use_multi_token_prediction: false
      num_future_tokens: 1
      mtp_weight: 0.01
      initial_temperature: 1.2
      adaptive_temperature: true
      label_smoothing: 0.08
      use_moe_balancing: true
      gradient_balance_weight: 0.02
      focal_loss: false
      contrastive_loss: false
      diversity_loss: true
      auxiliary_loss: true
      adaptive_loss_scaling: false
      contrastive_loss_weight: 0.02
      diversity_loss_weight: 0.1
    
    adaptive_mtp:
      use_adaptive_mtp: false
      num_prediction_heads: 3
      confidence_threshold_train: 0.6
      confidence_threshold_inference: 0.7
      gate_hidden_dims: 512,256
      gate_dropout: 0.1
      gate_activation: gelu
      use_attention_pooling: false
      head_type: linear
      head_intermediate_size: null
      head_dropout: 0.1
      share_projections: false
      mtp_warmup_epochs: 2
      confidence_reg_strength: 0.01
      use_confidence_weighting: true
      primary_loss_weight: 1
      additional_loss_base_weight: 0.1
      enable_dynamic_prediction: true
      min_confidence_for_computation: 0.3
    
    gradient:
      gradient_surgery: false
      adaptive_gradient_surgery: false
      gradient_surgery_method: pcgrad
      gradient_norm_tracking: false
      gradient_anomaly_detection: false
    
    memory:
      use_episodic_memory: false
      memory_capacity: 500
      memory_selection_strategy: importance
      memory_importance_threshold: 0.7
      memory_adaptation_rate: 0.01
    
    quantization:
      quantization_aware: false
      use_nvfp4: true
      use_torchao_nvfp4: true
      nvfp4_mode: weight_only
      bit_width: 4
      nvfp4_block_size: 16
      stochastic_rounding: false
      use_hadamard_transform: false
  
  deepspeed:
    use_deepspeed: true
    zero_stage: 1
    cpu_offload: false
    nvme_offload: false
    precision_type: fp16
    train_batch_size: 48  # UPDATED: Matches batch_size * gradient_accumulation_steps (12*4)
    micro_batch_size: 12  # UPDATED: Matches training.batch_size
    gradient_accumulation_steps: 4  # Matches training.gradient_accumulation_steps
    activation_checkpointing: true
  
  performance:
    ultra_fast_mode: true
    fast_progress: false
    minimal_progress: false
    express_mode: false
    no_sync: false
    
    torch_compile:
      enabled: true
      mode: max-autotune
      fullgraph: false
      dynamic: false
      backend: inductor
    
    torchinductor_max_autotune: '0'
    cudagraph_skip_dynamic_shapes: true
    cudagraph_dynamic_shape_warn_limit: null
    float32_matmul_precision: high
  
  memory:
    enable_memory_pool: true
    pool_size_gb: 24
    memory_threshold_gb: 24
    enable_cpu_offload: false
    clear_cache_frequency: 50
    silent_mode: true
    target_utilization: 0.90
    warning_threshold: 0.92
    critical_threshold: 0.95
    emergency_threshold: 0.97
  
  lr_finder:
    enabled: false
    start_lr: 1e-8
    end_lr: 0.01
    num_iterations: 1000
    suggestion_method: fastai
    use_suggested_lr: true
    use_savgol_filter: true
    savgol_window: 31
    beta: 0.9
    momentum_cycling: false
    track_validation: false
    num_runs: 1
  
  device: auto
  fp16: true
  bf16: false

data:
  data_dir: /project/code/data/processed
  max_length: 128
  tokenizer_name: /project/code/models/tokenizer/enhanced-65536
  padding_side: right
  truncation: true
  max_train_examples: null
  max_eval_examples: 5000
  dataloader_drop_last: true
  dataloader_pin_memory: false
  
  streaming: false
  buffer_size: 1000
  max_samples: null
  val_max_samples: 5000
  val_split_ratio: 0.15
  use_multi_column: false
  distributed: false
  num_workers: 6  # OPTIMIZED: Matches dataloader_num_workers for consistency
  samples_per_file: 100  # OPTIMIZED: Reduced file open/close overhead (was 1)
  
  min_sequence_length: 10
  max_sequence_repetition_rate: 0.6
  max_consecutive_repeats: 10
  skip_malformed_sequences: true
  
  enable_bucketing: true  # OPTIMIZED: ENABLED for 30-50% memory savings + 1.5-2x speedup
  bucket_boundaries:
    - 64
    - 128
  max_bucket_size: 32  # OPTIMIZED: Matches batch_size for optimal bucket flushing
  
  use_weighted_mixing: false
  mixing_temperature: 1
  shuffle_seed: 42
  reshuffle_each_epoch: true
  prefetch_factor: 4  # OPTIMIZED: Increased from 2 to eliminate GPU starvation
  persistent_workers: true
  
  format_detection_samples: 10
  
  fallback_data_paths:
    - /project/code/data/processed
    - /project/code/data/combined
    - /project/code/data
    - ./data/processed
    - ./data/combined
    - ./data
    - ../data/processed
    - ../data
    - ../../data

evaluation:
  eval_during_training: true
  eval_metrics: perplexity
  comprehensive_eval: false
  recent_losses_window_size: 100
  max_validation_batches: 100
  cache_clear_frequency: 50
  val_train_ratio_low_threshold: 0.8
  val_train_ratio_high_threshold: 1.05
  invalid_batch_rate_threshold: 0.2
  perplexity_overflow_threshold: 20
  max_nan_loss_logs: 5
  max_inf_loss_logs: 5
  max_invalid_batch_logs: 5
  num_test_batches: 3
  batch_size_variance_threshold: 2
  
  moe_metrics:
    track_expert_utilization: true
    track_routing_entropy: true
    track_load_balance: true
    track_expert_specialization: true
    log_frequency: 50

output:
  output_dir: outputs/small_enhanced
  save_total_limit: 3
  save_on_each_node: false
  gradient_checkpointing: true

run_management:
  experiment_name: ava-small-enhanced-v5-plateau-fix
  run_id: null
  resume_from_checkpoint: null

wandb:
  enabled: true
  project: Ava
  entity: null
  name: null
  group: ava-enhanced-v5-plateau-fix
  job_type: train
  tags:
    - gpu
    - rtx-3090-ti
    - 100M
    - enhanced
    - moe
    - custom-tokenizer
    - enhanced-65k
    - 65k-vocab
    - plateau-fix
    - adaptive-lr
    - v5
  resume_policy: allow
  save_code: true
  notes: |
    [PLATEAU FIX 2025-11-02] Addressing Training Plateau at Loss ~2.5:
    
    üéØ NEW OPTIMIZATIONS FOR PLATEAU BREAKING:
    
    1. LEARNING RATE IMPROVEMENTS:
       - learning_rate: 0.0001‚Üí0.00015 (50% higher for stronger updates)
       - lr_scheduler_type: cosine‚Üícosine_with_restarts (2 cycles for LR restarts)
       - lr_end: 0.00001‚Üí0.00003 (slower decay, 20% vs 10% of initial)
       - eval_steps: 1000‚Üí500 (more frequent eval to catch improvements)
    
    2. ADAPTIVE LR ENABLED:
       - enabled: false‚Üítrue (dynamic LR adjustment on plateaus)
       - plateau_patience: 1500 steps before adjustment
       - plateau_factor: 1.15 (boost LR by 15% when stuck)
       - max_lr: 0.0002 (can boost to 2x base LR)
       - min_improvement: 0.005 (require meaningful progress)
    
    3. BATCH SIZE OPTIMIZATION:
       - batch_size: 16‚Üí12 (stronger gradient signal)
       - gradient_accumulation_steps: 4 (same)
       - Effective batch: 64‚Üí48 (better for escaping local optima)
       - deepspeed configs updated to match (12*4=48)
    
    4. EARLY STOPPING ADJUSTMENTS:
       - patience: 3‚Üí5 (allow more time for improvement)
       - threshold: 0.01‚Üí0.005 (more sensitive to small gains)
    
    5. MONITORING IMPROVEMENTS:
       - eval_steps: 1000‚Üí500 (catch improvements faster)
       - moe_metrics.log_frequency: 50 (unchanged, still frequent)
    
    Expected Results:
    - Break through loss=2.5 plateau
    - Continue descent to loss=1.5-2.0
    - LR restarts at ~15k and ~30k steps
    - Adaptive LR kicks in if plateau persists
    - Smaller effective batch allows escape from local minima
    
    ‚úÖ STABILITY FIXES PRESERVED (from v4):
    - Batch size consistency maintained (12 across all configs)
    - Data pipeline simplified (streaming=false, bucketing=false)
    - MoE stability (8 experts, capacity=3.0, aux_loss=0.12)
    - Precision optimized (FP16 for small models)
    - Regularization balanced (dropout=0.12, weight_decay=0.08)
    
    Previous runs analysis:
    - v4: Stable training, eliminated spikes ‚úÖ
    - v4: Plateau at loss ~2.5 from 10k-30k steps ‚ö†Ô∏è
    - v5: Addressing plateau with LR schedule + adaptive LR
    
    Target metrics:
    - Final loss: 1.5-2.0 (from current 2.5)
    - Perplexity: <7.4 (currently ~12.2)
    - Smooth continued descent without plateau
  offline: false
  cache_size: 10000
  cache_flush_interval: 500

stability:
  loss_smoothing: true
  loss_smoothing_alpha: 0.95
  lr_warmup_ratio: 0.0333
  lr_decay_ratio: 0.1
  sync_gradients_each_step: false
  use_kahan_summation: true
  use_stable_embedding: true
  stop_on_nan: false
  max_grad_norm_before_stop: 100
  max_loss_before_stop: 20

generation:
  repetition_penalty: 3
  eos_penalty: 0.5
  min_length: 40
  temperature: 1
  top_k: 100
  top_p: 0.95
  use_ngram_blocking: true
  ngram_size: 3
  no_repeat_ngram_size: 4
  do_sample: true
  num_beams: 1
  eval_max_length: 50
  eval_temperature: 0.8
  eval_top_p: 0.9
  tokenizer_max_length: 128
  min_tokens_for_trigrams: 4
  trigram_size: 3
  sample_display_max_chars: 100
  coherence_excellent_threshold: 75
  coherence_moderate_threshold: 50
  distinct_2_threshold: 0.7
  repetition_threshold: 0.3
  entropy_threshold: 4

rlhf:
  policy_model_path: /project/code/outputs/runs/latest/model
  judge_model_path: /project/code/outputs/runs/latest/model
  reward_model_path: null
  use_model_to_model_reward: true
  freeze_reward_model: true
  prompt_dataset_path: /project/code/data/rlhf/prompts.json
  eval_prompt_dataset_path: /project/code/data/rlhf/eval_prompts.json
  num_epochs: 3
  rollout_batch_size: 16
  num_rollouts_per_epoch: 500
  max_prompt_length: 128
  save_dir: outputs/rlhf/small
  log_dir: logs/rlhf/small
  save_every: 500
  eval_every: 250
  num_eval_prompts: 50
  device: cuda
  use_wandb: true
  wandb_project: ava-rlhf
  wandb_name: rlhf-small-v1
  
  ppo:
    learning_rate: 5e-7
    batch_size: 32
    mini_batch_size: 4
    max_grad_norm: 0.5
    ppo_epochs: 4
    clip_range: 0.2
    clip_range_value: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    gamma: 1
    lambda: 0.95
    kl_penalty: kl
    target_kl: 0.01
    init_kl_coef: 0.2
    adaptive_kl: true
    max_gen_length: 128
    num_train_epochs: 1
    max_steps: null
    warmup_steps: 50
    save_steps: 500
    eval_steps: 250
    use_score_scaling: true
    use_score_norm: true
    whiten_rewards: true
    clip_reward: 10