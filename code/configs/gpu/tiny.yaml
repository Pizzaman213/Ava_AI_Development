model:
  hidden_size: 384
  num_layers: 8
  num_attention_heads: 6
  intermediate_size: 1536
  vocab_size: 65536  # Enhanced custom tokenizer vocab size
  max_position_embeddings: 1024
  num_experts: 4
  num_experts_per_token: 2
  expert_capacity_factor: 1.5
  router_type: switch
  router_aux_loss_coef: 0.01
  router_jitter_noise: 0.0
  router_z_loss_coef: 0.0001
  moe_dropout: 0.0
  use_cache: true
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  layer_norm_eps: 1e-6
  initializer_range: 0.02
  hidden_act: swiglu
  label_smoothing: 0.0
  tie_word_embeddings: false
  dropout: 0.0
  use_moh: false
  use_moa: false
  use_rag: false
  use_cross_attention: false
  use_episodic_memory: false
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: false
  deepspeed_moe_param_groups: true
  use_flash_attention: true
training:
  batch_size: 16
  gradient_accumulation_steps: 1
  max_gradient_norm: 5.0
  num_epochs: 1
  max_steps: 50000  # CRITICAL FIX: Reduced from 500k to prevent over-training (~15-20 epochs)
  warmup_steps: 2000
  warmup_schedule: linear
  save_total_limit: 20
  learning_rate: 0.00042
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  lr_scheduler_type: cosine
  lr_end: 1.0e-04  # CRITICAL FIX: Increased from 7.5e-06 to prevent LR collapse (13x higher)
  # Repetition penalty weights (Fix #2 - addresses " time time time" issue)
  repetition_penalty_weight: 0.5      # N-gram repetition penalty weight
  immediate_repetition_weight: 1.0    # Token-to-token repetition weight
  min_sequence_length: 20             # Minimum tokens before allowing EOS
  eos_penalty_weight: 5.0             # Penalty for early EOS tokens
  warmup_ratio: 0.01
  mixed_precision: bf16
  dataloader_num_workers: 12
  gradient_checkpointing: true
  use_adaptive_lr: false
  lr_patience: 1000000
  lr_reduction_factor: 1.0
  min_lr: 0.0
  eval_strategy: steps
  eval_steps: 2000
  save_strategy: steps
  save_steps: 3000
  logging_steps: 200
  load_best_model_at_end: false
  metric_for_best_model: loss
  greater_is_better: false
  early_stopping_patience: 1000
  early_stopping_threshold: 0.0001
  min_loss_reduction: 0.0001
  use_ema: false
  clip_grad_value: null
  loss_scaling: dynamic
  adam_epsilon: 1e-8
  gradient_health:
    enabled: false
    initial_clip_value: 5.0
    final_clip_value: 2.0
    warmup_steps: 2000  # Matches training.warmup_steps for consistency
    explosion_threshold: 30.0
    explosion_window: 5
    auto_reduce_lr: false
    lr_reduction_factor: 0.5
  dynamic_batching:
    enabled: false
    min_batch_size: 8
    max_batch_size: 48
    target_memory_utilization: 0.88
    adjustment_frequency: 100
    adjustment_factor: 1.3
    warmup_steps: 2000  # Matches training.warmup_steps for consistency
    smooth_transitions: true
data:
  data_dir: /project/code/data/processed
  max_length: 2048
  max_train_examples: -1
  max_eval_examples: 1000
  tokenizer_name: /project/code/models/tokenizer/enhanced-65k  # Custom enhanced tokenizer
  padding_side: right
  truncation: true
  train_batch_size: 16
  eval_batch_size: 24
  dataloader_drop_last: true
  dataloader_pin_memory: false
  shuffle: true

  # Data loading configuration
  streaming: true
  buffer_size: 15000
  max_samples: -1
  shuffle_seed: 42
  reshuffle_each_epoch: true
  drop_last_batch: true
  prefetch_factor: 2
  persistent_workers: true
  multi_column: false
  use_multi_column: false
  distributed: false
  use_weighted_mixing: true
  mixing_temperature: 1.0
deepspeed:
  use_deepspeed: false
  zero_stage: 0
  cpu_offload: false
  nvme_offload: false
  precision_type: bf16
  train_batch_size: 16
  micro_batch_size: 16
  gradient_accumulation_steps: 1
  activation_checkpointing: true
enhanced_features:
  architecture:
    use_moh: false
    use_moa: false
    use_cross_attention: false
    use_alibi: false
    expert_routing_type: switch
  rag:
    enabled: false
    knowledge_base_path: null
    max_retrieved_docs: 5
    rag_fusion_type: concatenate
  losses:
    focal_loss: false
    contrastive_loss: false
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true
  gradient:
    gradient_surgery: false
    adaptive_gradient_surgery: false
    gradient_surgery_method: pcgrad
    gradient_norm_tracking: false
    gradient_anomaly_detection: false
  memory:
    use_episodic_memory: false
    memory_capacity: 500
    memory_selection_strategy: importance
    memory_importance_threshold: 0.7
    memory_adaptation_rate: 0.01
  quantization:
    quantization_aware: false
    use_nvfp4: false
    bit_width: 8
    nvfp4_block_size: 64
    stochastic_rounding: falseperformance:
  ultra_fast_mode: true
  fast_progress: false
  minimal_progress: false
  express_mode: false
  no_sync: false
memory:
  enable_memory_pool: true
  pool_size_gb: 8.0
  gradient_checkpointing: true
  memory_threshold_gb: 8.0
  enable_cpu_offload: false
  clear_cache_frequency: 200
evaluation:
  eval_during_training: true
  eval_metrics: perplexity
  comprehensive_eval: false
output:
  output_dir: outputs/tiny
  save_total_limit: 3
  save_on_each_node: false
run_management:
  experiment_name: ava-tiny-enhanced
  run_id: null
  resume_from_checkpoint: null
device: auto
fp16: false
bf16: true
wandb:
  enabled: true
  project: ava-moe-gpu-tiny-ultrafast
  tags:
  - gpu
  - tiny
  - 100M
  - ultrafast
  - optimized
  - production
  - custom-tokenizer
  - enhanced-65k
  - 65k-vocab
  - weighted-mixing
  - pre-tokenized
  entity: null
  name: null
  notes: 'Enhanced with: Custom 65k tokenizer (math/code optimized), weighted data mixing
    (DoReMi), pre-tokenized data'
  group: tiny-models-ultrafast
  job_type: train
  offline: false
  cache_size: 5000
  cache_flush_interval: 200
stability:
  loss_smoothing: true
  loss_smoothing_alpha: 0.9
  lr_warmup_ratio: 0.1
  lr_decay_ratio: 0.1
  sync_gradients_each_step: false
  use_kahan_summation: true
  use_stable_embedding: true
  stop_on_nan: true
  max_grad_norm_before_stop: 100.0
  max_loss_before_stop: 20.0
generation:
  repetition_penalty: 1.2
  eos_penalty: 0.5
  min_length: 20
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  use_ngram_blocking: false
  ngram_size: 3
  no_repeat_ngram_size: 0
  do_sample: true
  num_beams: 1
lr_finder:
  enabled: false
  start_lr: 1.0e-08
  end_lr: 0.01
  num_iterations: 500
  suggestion_method: fastai
  use_suggested_lr: false
  use_savgol_filter: true
  savgol_window: 31
  beta: 0.9
  momentum_cycling: false
  track_validation: false
  num_runs: 1
  gradient_accumulation_steps: 1

# RLHF Fine-tuning Configuration
rlhf:
  # Model paths
  policy_model_path: /project/code/outputs/runs/latest/model  # Path to pretrained model to finetune
  judge_model_path: /project/code/outputs/runs/latest/model  # Path to judge model (can be same as policy)
  reward_model_path: null  # Optional: path to pre-trained reward model

  # Reward model settings
  use_model_to_model_reward: true  # Use model-to-model rating (judge evaluates policy outputs)
  freeze_reward_model: true  # Freeze reward model during training

  # Dataset paths
  prompt_dataset_path: /project/code/data/rlhf/prompts.json  # Path to prompts dataset
  eval_prompt_dataset_path: /project/code/data/rlhf/eval_prompts.json  # Optional: evaluation prompts

  # Training settings
  num_epochs: 3
  rollout_batch_size: 24  # Number of prompts to process at once
  num_rollouts_per_epoch: 500  # Number of rollouts per epoch
  max_prompt_length: 512

  # Output settings
  save_dir: outputs/rlhf/tiny
  log_dir: logs/rlhf/tiny
  save_every: 500  # Save checkpoint every N steps
  eval_every: 250  # Evaluate every N steps
  num_eval_prompts: 50  # Number of prompts to use for evaluation

  # Device
  device: cuda  # cuda or cpu

  # Weights & Biases logging
  use_wandb: true
  wandb_project: ava-rlhf
  wandb_name: rlhf-tiny-v1

  # PPO configuration
  ppo:
    # Learning rate and optimization
    learning_rate: 1.0e-6  # Low LR for RLHF fine-tuning
    batch_size: 24  # PPO batch size
    mini_batch_size: 6  # Mini-batch size for PPO updates
    gradient_accumulation_steps: 2  # Gradient accumulation
    max_grad_norm: 0.5  # Lower gradient clipping for stability

    # PPO hyperparameters
    ppo_epochs: 4  # Number of PPO epochs per batch
    clip_range: 0.2  # PPO clipping parameter
    clip_range_value: 0.2  # Value function clipping
    value_loss_coef: 0.5  # Weight for value loss
    entropy_coef: 0.01  # Entropy bonus coefficient
    gamma: 1.0  # Discount factor (1.0 for language modeling)
    lambda: 0.95  # GAE lambda

    # KL divergence penalty
    kl_penalty: kl  # 'kl', 'abs', or 'mse'
    target_kl: 0.01  # Target KL divergence
    init_kl_coef: 0.2  # Initial KL coefficient
    adaptive_kl: true  # Adaptively adjust KL coefficient

    # Generation settings
    max_gen_length: 128  # Maximum generation length
    temperature: 1.0  # Sampling temperature
    top_k: 50  # Top-k sampling
    top_p: 0.95  # Nucleus sampling

    # Training schedule
    num_train_epochs: 1
    max_steps: null  # Optional: maximum number of steps
    warmup_steps: 100  # Warmup steps
    logging_steps: 10
    save_steps: 500
    eval_steps: 250

    # Reward processing
    use_score_scaling: true  # Scale rewards
    use_score_norm: true  # Normalize rewards
    whiten_rewards: true  # Whiten advantages
    clip_reward: 10.0  # Clip rewards to [-clip_reward, clip_reward]

    # Memory optimization
    gradient_checkpointing: true  # Tiny model doesn't need checkpointing
    mixed_precision: bf16  # 'no', 'fp16', or 'bf16'
