# NVFP4 Quantization Research Configuration
# Specialized config for testing 4-bit quantization with NVFP4
# Focus on extreme memory efficiency and quantization quality

model:
  # Architecture optimized for quantization research
  hidden_size: 1024
  num_layers: 20
  num_attention_heads: 16
  intermediate_size: 4096
  vocab_size: 50257
  max_position_embeddings: 2048

  # MoE Configuration (larger for quantization benefits)
  num_experts: 32
  num_experts_per_token: 4
  expert_capacity_factor: 1.5
  router_type: "switch"

  # Features (focus on what works well with quantization)
  use_moh: true
  use_moa: false  # Can interfere with quantization
  use_rag: false
  use_cross_attention: false
  use_episodic_memory: true
  memory_size: 1000

training:
  batch_size: 4
  gradient_accumulation_steps: 8
  max_gradient_norm: 1.0
  num_epochs: 6
  warmup_steps: 2000
  learning_rate: 8e-5  # Lower LR for quantization stability
  weight_decay: 0.01
  mixed_precision: "bf16"

# Enhanced Features (Quantization-focused)
enhanced_features:
  architecture:
    use_moh: true
    use_moa: false
    use_cross_attention: false
    expert_routing_type: "switch"

  # RAG disabled for quantization focus
  rag:
    enabled: false

  # Losses (stable for quantization)
  losses:
    focal_loss: false
    contrastive_loss: false
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true

  # Gradient surgery (can help with quantization instability)
  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: false
    gradient_surgery_method: "pcgrad"

  # Memory (complementary to quantization)
  memory:
    use_episodic_memory: true
    memory_capacity: 1000
    memory_selection_strategy: "importance"

  # Quantization (primary focus)
  quantization:
    quantization_aware: true
    use_nvfp4: true              # Primary research focus
    bit_width: 4                 # 4-bit quantization
    nvfp4_block_size: 64         # Block size for NVFP4
    stochastic_rounding: true    # Improves quantization quality
    use_hadamard_transform: true # Advanced NVFP4 feature

    # NVFP4 specific settings
    nvfp4_calibration_samples: 1000
    nvfp4_outlier_threshold: 6.0
    nvfp4_dynamic_quantization: true
    nvfp4_per_channel: true

    # Quantization schedule
    start_quantization_step: 1000  # Warmup before quantization
    quantization_schedule: "linear"
    quantization_warmup_steps: 500

# DeepSpeed (compatible with quantization)
deepspeed:
  enabled: false  # NVFP4 might not work with all DeepSpeed features
  zero_stage: 1
  precision: "bf16"

# Data Loading
data:
  data_dir: "/project/code/data/processed"
  max_length: 2048
  tokenizer_name: "gpt2"
  train_batch_size: 4
  eval_batch_size: 8

  # Data loading configuration
  streaming: true
  buffer_size: 5000
  distributed: false

# Performance (optimized for quantization overhead)
performance:
  ultra_fast_mode: false
  fast_progress: true
  minimal_progress: false
  express_mode: false  # Disable for accurate quantization timing
  no_sync: false

# Memory (aggressive due to quantization)
memory:
  enable_memory_pool: true
  pool_size_gb: 16.0
  gradient_checkpointing: true
  memory_threshold_gb: 16.0
  clear_cache_frequency: 50

# Evaluation (quantization-focused metrics)
evaluation:
  eval_during_training: true
  eval_metrics: "perplexity,quantization_error,model_size,inference_speed"
  comprehensive_eval: true

  # Quantization-specific evaluation
  eval_quantization_quality: true
  eval_memory_usage: true
  eval_speed_improvement: true
  compare_fp16_performance: true

# Output
output:
  output_dir: "outputs/nvfp4_research"
  save_total_limit: 3
  save_quantized_model: true
  save_calibration_data: true

# Run Management
run_management:
  experiment_name: "ava-nvfp4-quantization"
  run_id: null

# Monitoring
wandb:
  enabled: true
  project: "ava-moe-nvfp4-research"
  tags: ["research", "quantization", "nvfp4", "4bit", "memory-efficiency"]
  notes: "NVFP4 4-bit quantization research with MoE models"
  group: "quantization-research"
  job_type: "research"

  # Track quantization metrics
  log_quantization_histograms: true
  log_weight_distributions: true

# Hardware Requirements
# VRAM: 12GB+ (significantly reduced due to quantization)
# Compatible GPUs: H100, A100 (for NVFP4 support)
# Use case: Quantization research, memory-constrained deployment
# Benefits: ~4x memory reduction, potential speedup, maintained quality