# A100 80GB Optimized Configuration
# Hardware-specific config optimized for NVIDIA A100 80GB
# Maximizes utilization of 80GB HBM2e memory and Tensor Cores

model:
  # Large architecture to utilize A100 memory
  hidden_size: 1280
  num_layers: 28
  num_attention_heads: 20
  intermediate_size: 5120
  vocab_size: 50257
  max_position_embeddings: 4096

  # MoE Configuration (aggressive for 80GB)
  num_experts: 48
  num_experts_per_token: 6
  expert_capacity_factor: 2.0
  router_type: "gshard"

  # Enhanced Features (all enabled for A100)
  use_moh: true
  use_moa: true
  use_rag: true
  use_cross_attention: true
  num_cross_attention_layers: 6
  use_episodic_memory: true
  memory_size: 3000

  # A100-specific optimizations
  deepspeed_activation_checkpointing: true
  deepspeed_partition_activations: true
  deepspeed_moe_param_groups: true

training:
  batch_size: 4     # Larger batch for A100
  gradient_accumulation_steps: 8
  max_gradient_norm: 1.0
  num_epochs: 5
  warmup_steps: 3000
  learning_rate: 6e-5
  weight_decay: 0.01
  mixed_precision: "bf16"  # A100 excels at BF16
  gradient_checkpointing: true

# DeepSpeed (optimized for A100)
deepspeed:
  enabled: true
  zero_stage: 2
  cpu_offload: false       # Not needed with 80GB
  nvme_offload: false
  precision: "bf16"        # A100 optimized
  gradient_accumulation_steps: 8
  train_batch_size: 32
  micro_batch_size: 4
  activation_checkpointing: true
  partition_activations: true

# Enhanced Features (A100 can handle everything)
enhanced_features:
  architecture:
    use_moh: true
    use_moa: true
    use_cross_attention: true
    use_alibi: true
    expert_routing_type: "gshard"

  rag:
    enabled: true
    knowledge_base_path: "data/knowledge_base"
    max_retrieved_docs: 15
    rag_fusion_type: "attention"

  losses:
    focal_loss: true
    contrastive_loss: true
    diversity_loss: true
    auxiliary_loss: true
    adaptive_loss_scaling: true

  gradient:
    gradient_surgery: true
    adaptive_gradient_surgery: true
    gradient_surgery_method: "cagrad"

  memory:
    use_episodic_memory: true
    memory_capacity: 3000
    memory_selection_strategy: "importance"
    memory_replay_ratio: 0.3

  quantization:
    quantization_aware: false  # Not needed with 80GB
    use_nvfp4: false

# Data Loading (A100 optimized)
data:
  data_dir: "/project/code/data/processed"
  max_length: 4096        # Longer sequences for A100
  tokenizer_name: "gpt2"
  train_batch_size: 4
  eval_batch_size: 8
  dataloader_num_workers: 16  # A100 can handle more workers

  # Data loading configuration
  streaming: true
  buffer_size: 20000
  distributed: true
  multi_column: true

# Performance (A100 optimized)
performance:
  ultra_fast_mode: false
  fast_progress: false
  minimal_progress: true
  express_mode: false
  no_sync: false

# Memory (aggressive utilization of 80GB)
memory:
  enable_memory_pool: true
  pool_size_gb: 70.0      # Use most of 80GB
  gradient_checkpointing: true
  memory_threshold_gb: 70.0
  enable_cpu_offload: false
  clear_cache_frequency: 25

  # A100-specific memory optimizations
  a100_memory_optimization: true
  use_memory_efficient_attention: true
  optimize_for_a100: true

# Compilation (A100 benefits from compilation)
compilation:
  enabled: true
  backend: "inductor"
  fullgraph: false
  dynamic: true

# Evaluation
evaluation:
  eval_during_training: true
  eval_metrics: "perplexity,bleu,rouge,bertscore"
  comprehensive_eval: true

# Output
output:
  output_dir: "outputs/a100_80gb"
  save_total_limit: 2

# Run Management
run_management:
  experiment_name: "ava-a100-80gb-optimized"

# Monitoring
wandb:
  enabled: true
  project: "ava-moe-a100-80gb"
  tags: ["hardware", "a100", "80gb", "optimized", "bf16", "tensor-cores"]
  notes: "A100 80GB optimized configuration with maximum feature utilization"
  group: "a100-optimized"

# Hardware-Specific Settings
hardware:
  gpu_type: "A100-SXM4-80GB"
  tensor_cores: true
  bf16_support: true
  memory_bandwidth: "2TB/s"
  nvlink_support: true

# Hardware Requirements
# GPU: NVIDIA A100 80GB (SXM4 or PCIe)
# Memory: 80GB HBM2e
# Compute Capability: 8.0
# Tensor Cores: 3rd Gen
# Use case: Maximum performance training with all features enabled